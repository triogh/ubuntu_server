<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title></title>
<meta name="generator" content="HTML::TextToHTML v2.51"/>
</head>
<body>
<p>###################################################
## Wed Nov  2 09:53:53 CET 2016<br/>
## <a href="https://www.digitalocean.com/community/tutorials/how-to-use-the-digitalocean-docker-application">https://www.digitalocean.com/community/tutorials/how-to-use-the-digitalocean-docker-application</a>
## PostedSeptember 20, 2013<br/>
###################################################
</p>
<p>$ docker run ubuntu echo "Hello World"<br/>
-docker will search for the 'ubuntu' image on your local machine, if it doesn't find it it will pull the image from the 'Docker Central Registry'
-docker will run a container from an image named "ubuntu" (which contains the whole "ubuntu" root file system ~180MB)
-docker will execute 'echo "Hello World"' in the shell (in the container) and exit
</p>
<p>$ docker images<br/>
-list docker images which have been downloaded/cached on your local machine
</p>
<p>$ docker ps -a<br/>
-lists all containers, even those which have exited (are not running any more)
-those which have exited are visible because the writable file system layer used by the temporary container (for example the one mentioned above, which echo-ed "Hello World" and exited) is still around
</p>
<p>$ docker rm<br/>
-remove a container
</p>
<p>$ docker rmi<br/>
-remove an image
</p>
<p>$ docker version<br/>
-shows currently installed docker client/server version
<strong>## END</strong>
</p>
<p>###################################################
## Wed Nov  2 10:39:28 CET 2016<br/>
## <a href="https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-getting-started">https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-getting-started</a>
## PostedDecember 11, 2013<br/>
###################################################
</p>
<p>Docker is here to offer you an efficient, speedy way to port applications across systems and machines. It is light and lean, allowing you to quickly contain applications and run them within their own secure environments (via Linux Containers: LXC). 
</p>
<p>Whether it be from your development machine to a remote server for production, or packaging everything for use elsewhere, it is always a challenge when it comes to porting your application stack together with its dependencies and getting it to run without hiccups. In fact, the challenge is immense and solutions so far have not really proved successful for the masses.
</p>
<p>In a nutshell, docker as a project offers you the complete set of higher-level tools to carry everything that forms an application across systems and machines - virtual or physical - and brings along loads more of great benefits with it.
</p>
<p>Docker achieves its robust application (and therefore, process and resource) containment via Linux Containers (e.g. namespaces and other kernel features). Its further capabilities come from a project's own parts and components, which extract all the complexity of working with lower-level linux tools/APIs used for system and application management with regards to securely containing processes.
</p>
<p>Main Docker Parts:
</p>
<p>docker daemon: used to manage docker (LXC) containers on the host it runs
docker CLI: used to command and communicate with the docker daemon
docker image index: a repository (public or private) for docker images
</p>
<p>Main Docker Elements:
</p>
<p>docker containers: directories containing everything-your-application
docker images: snapshots of containers or base OS (e.g. Ubuntu) images
Dockerfiles: scripts automating the building process of images
</p>
<p>Docker Containers:
</p>
<p>The entire procedure of porting applications using docker relies solely on the shipment of containers.
</p>
<p>Docker containers are basically directories which can be packed (e.g. tar-archived) like any other, then shared and run across various different machines and platforms (hosts). The only dependency is having the hosts tuned to run the containers (i.e. have docker installed). Containment here is obtained via Linux Containers (LXC).
</p>
<p>Linux Containers can be defined as a combination various kernel-level features (i.e. things that Linux-kernel can do) which allow management of applications (and resources they use) contained within their own environment. By making use of certain features (e.g. namespaces, chroots, cgroups and SELinux profiles), the LXC contains application processes and helps with their management through limiting resources, not allowing reach beyond their own file-system (access to the parent's namespace) etc.
</p>
<p>Docker with its containers makes use of LXC, however, also brings along much more.
</p>
<p>Being based and depending on LXC, from a technical aspect, these containers are like a directory (but a shaped and formatted one). This allows portability and gradual builds of containers.
</p>
<p>Each container is layered like an onion and each action taken within a container consists of putting another block (which actually translates to a simple change within the file system) on top of the previous one. And various tools and configurations make this set-up work in a harmonious way altogether (e.g. union file-system/aufs).
</p>
<p>What this way of having containers allows is the extreme benefit of easily launching and creating new containers and images, which are thus kept lightweight (thanks to gradual and layered way they are built). Since everything is based on the file-system, taking snapshots and performing roll-backs in time are cheap (i.e. very easily done / not heavy on resources), much like version control systems (VCS).
</p>
<p>Each docker container starts from a docker image which forms the base for other applications and layers to come.
</p>
<p>Docker Images:
</p>
<p>Docker images constitute the base of docker containers from which everything starts to form. They are very similar to default operating-system disk images which are used to run applications on servers or desktop computers.
</p>
<p>Having these images (e.g. Ubuntu base) allow seamless portability across systems. They make a solid, consistent and dependable base with everything that is needed to run the applications. When everything is self-contained and the risk of system-level updates or modifications are eliminated, the container becomes immune to external exposures which could put it out of order - preventing the dependency hell.
</p>
<p>As more layers (tools, applications etc.) are added on top of the base, new images can be formed by committing these changes. When a new container gets created from a saved (i.e. committed) image, things continue from where they left off. And the union file system, brings all the layers together as a single entity when you work with a container.
</p>
<p>These base images can be explicitly stated when working with the docker CLI to directly create a new container or they might be specified inside a Dockerfile for automated image building. 
</p>
<dl>
  <dt>Dockerfiles</dt>
<dd>
</dd></dl>
<p>Dockerfiles are scripts containing a successive series of instructions, directions, and commands which are to be executed to form a new docker image. Each command executed translates to a new layer of the onion, forming the end product. They basically replace the process of doing everything manually and repeatedly. When a Dockerfile is finished executing, you end up having formed an image, which then you use to start (i.e. create) a new container.
</p>
<p>$ docker<br/>
-lists all available docker commands
</p>
<p>$ docker info<br/>
-shows system-wide information on docker
-for example total number of created containers, which of them are Running/Paused/Stopped
-number of locally available images<br/>
-storage driver, i.e. aufs<br/>
-root directory, i.e. /var/lib/docker/aufs
-Network, i.e. host null overlay bridge
-Docker Root Dir, i.e. /var/lib/docker
</p>
<p>Working with Images:
</p>
<p>As we have discussed at length, the key to start working with any docker container is using images. There are many freely available images shared across docker image index and the CLI allows simple access to query the image repository and to download new ones.
</p>
<p>When you are ready, you can also share your image there as well. See the section on &acirc;Äúpush&acirc;Äù further down for details.
</p>
<p>$ docker search ubuntu<br/>
-searching for a docker image, in the above example, the image we are searching for is named ubuntu
-this search will provide you a list of all available images matching the query 'ubuntu'
</p>
<p>$ docker pull ubuntu<br/>
-either when you are building / creating a container or before you do, you will need to have an image present at the host machine where the containers will exist. In order to download images (perhaps following &acirc;Äúsearch&acirc;Äù) you can execute pull to get one
</p>
<p>$ docker images<br/>
-all the images on your system, including the ones you have created by committing (see below for details), can be listed using &acirc;Äúimages&acirc;Äù
</p>
<p>$ docker commit 8dbd9e392a96 my_img<br/>
-as you work with a container and continue to perform actions on it (e.g. download and install software, configure files etc.), to have it keep its state, you need to &acirc;Äúcommit&acirc;Äù 
-committing makes sure that everything continues from where they left next time you use one (i.e. an image)
</p>
<p>$ docker push my_username/my_first_image
-although it is a bit early at this moment - in our article, when you have created your own container which you would like to share with the rest of the world, you can use push to have your image listed in the index where everybody can download and use
-please remember to &acirc;Äúcommit&acirc;Äù all your changes
-note: You need to sign-up at index.docker.io to push images to docker index
</p>
<p>Working with Containers:
</p>
<p>When you "run" any process using an image, in return, you will have a container. When the process is not actively running, this container will be a non-running container. Nonetheless, all of them will reside on your system until you remove them via rm command.
</p>
<p>$ docker ps<br/>
-list all running containers
</p>
<p>$ docker ps -a<br/>
-list of both running and non-running ones
</p>
<p>$ docker ps -l<br/>
-list the last run container<br/>
-shows only the latest created container, include non-running ones
</p>
<p>Creating a New Container:
</p>
<p>It is currently not possible to create a container without running anything (i.e. commands). To create a new container, you need to use a base image and specify a command to run.
</p>
<p>$ docker run my_img echo "hello"<br/>
-usage: docker run [image name] [command to run]
</p>
<p>$ docker run -name my_cont_1 my_img echo "hello"
-to name a container instead of having long IDs
-usage: sudo docker run -name [name] [image name] [command to run]
</p>
<p>Running a container:
</p>
<p>$ docker run c629b7d70666<br/>
-usage: docker run [container ID]<br/>
-when you create a container and it stops (either due to its process ending or you stopping it explicitly), you can use &acirc;Äúrun&acirc;Äù to get the container working again with the same command used to create it.
</p>
<p>Stopping a container:
</p>
<p>$ docker stop c629b7d70666<br/>
-usage: docker stop [container ID]<br/>
-to stop a container's process from running
</p>
<p>Saving (committing) a container:
</p>
<p>If you would like to save the progress and changes you made with a container, you can use &acirc;Äúcommit&acirc;Äù as explained above to save it as an image.
</p>
<p>    This command turns your container to an image.
</p>
<p>Remember that with docker, commits are cheap. Do not hesitate to use them to create images to save your progress with a container or to roll back when you need (e.g. like snapshots in time).
</p>
<p>Removing / Deleting a container:<br/>
$ docker rm c629b7d70666<br/>
-usage: docker rm [container ID]<br/>
-using the ID of a container, you can delete one with rm</p>

</body>
</html>
