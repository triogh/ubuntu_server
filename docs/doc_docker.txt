Docker Documentation
***

Title: how-to-use-the-digitalocean-docker-application
===
Source: https://www.digitalocean.com/community/tutorials/how-to-use-the-digitalocean-docker-application

Added: Wed Nov  2 09:53:53 CET 2016
Created: PostedSeptember 20, 2013


$ docker run ubuntu echo "Hello World"
+++
-docker will search for the 'ubuntu' image on your local machine, if it doesn't find it it will pull the image from the 'Docker Central Registry'

-docker will run a container from an image named "ubuntu" (which contains the whole "ubuntu" root file system ~180MB)

-docker will execute 'echo "Hello World"' in the shell (in the container) and exit


$ docker images
+++
-list docker images which have been downloaded/cached or created on your local machine


$ docker ps -a
+++
-lists all containers, even those which have exited (are not running any more)

-those which have exited are visible because the writable file system layer used by the temporary container (for example the one mentioned above, which echo-ed "Hello World" and exited) is still around


$ docker rm
+++
-remove a container


$ docker rmi
+++
-remove an image


$ docker version
+++
-shows currently installed docker client/server version



Title: how-to-install-and-use-docker-getting-started
===
Source: https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-getting-started

Added: Wed Nov  2 10:39:28 CET 2016
Created: PostedDecember 11, 2013


Docker is here to offer you an efficient, speedy way to port applications across systems and machines. It is light and lean, allowing you to quickly contain applications and run them within their own secure environments (via Linux Containers: LXC). 

Whether it be from your development machine to a remote server for production, or packaging everything for use elsewhere, it is always a challenge when it comes to porting your application stack together with its dependencies and getting it to run without hiccups. In fact, the challenge is immense and solutions so far have not really proved successful for the masses.

In a nutshell, docker as a project offers you the complete set of higher-level tools to carry everything that forms an application across systems and machines - virtual or physical - and brings along loads more of great benefits with it.

Docker achieves its robust application (and therefore, process and resource) containment via Linux Containers (e.g. namespaces and other kernel features). Its further capabilities come from a project's own parts and components, which extract all the complexity of working with lower-level linux tools/APIs used for system and application management with regards to securely containing processes.

Main Docker Parts:
+++

- docker daemon: used to manage docker (LXC) containers on the host it runs
- docker CLI: used to command and communicate with the docker daemon
- docker image index: a repository (public or private) for docker images

Main Docker Elements:
+++

- docker containers: directories containing everything-your-application
- docker images: snapshots of containers or base OS (e.g. Ubuntu) images
- Dockerfiles: scripts automating the building process of images


Docker Containers:
+++

The entire procedure of porting applications using docker relies solely on the shipment of containers.

Docker containers are basically directories which can be packed (e.g. tar-archived) like any other, then shared and run across various different machines and platforms (hosts). The only dependency is having the hosts tuned to run the containers (i.e. have docker installed). Containment here is obtained via Linux Containers (LXC).

Linux Containers can be defined as a combination various kernel-level features (i.e. things that Linux-kernel can do) which allow management of applications (and resources they use) contained within their own environment. By making use of certain features (e.g. namespaces, chroots, cgroups and SELinux profiles), the LXC contains application processes and helps with their management through limiting resources, not allowing reach beyond their own file-system (access to the parent's namespace) etc.

Docker with its containers makes use of LXC, however, also brings along much more.

Being based and depending on LXC, from a technical aspect, these containers are like a directory (but a shaped and formatted one). This allows portability and gradual builds of containers.

Each container is layered like an onion and each action taken within a container consists of putting another block (which actually translates to a simple change within the file system) on top of the previous one. And various tools and configurations make this set-up work in a harmonious way altogether (e.g. union file-system/aufs).

What this way of having containers allows is the extreme benefit of easily launching and creating new containers and images, which are thus kept lightweight (thanks to gradual and layered way they are built). Since everything is based on the file-system, taking snapshots and performing roll-backs in time are cheap (i.e. very easily done / not heavy on resources), much like version control systems (VCS).

Each docker container starts from a docker image which forms the base for other applications and layers to come.


Docker Images:
+++

Docker images constitute the base of docker containers from which everything starts to form. They are very similar to default operating-system disk images which are used to run applications on servers or desktop computers.

Having these images (e.g. Ubuntu base) allow seamless portability across systems. They make a solid, consistent and dependable base with everything that is needed to run the applications. When everything is self-contained and the risk of system-level updates or modifications are eliminated, the container becomes immune to external exposures which could put it out of order - preventing the dependency hell.

As more layers (tools, applications etc.) are added on top of the base, new images can be formed by committing these changes. When a new container gets created from a saved (i.e. committed) image, things continue from where they left off. And the union file system, brings all the layers together as a single entity when you work with a container.

These base images can be explicitly stated when working with the docker CLI to directly create a new container or they might be specified inside a Dockerfile for automated image building.


Dockerfiles
+++

Dockerfiles are scripts containing a successive series of instructions, directions, and commands which are to be executed to form a new docker image. Each command executed translates to a new layer of the onion, forming the end product. They basically replace the process of doing everything manually and repeatedly. When a Dockerfile is finished executing, you end up having formed an image, which then you use to start (i.e. create) a new container.


$ docker
---
-lists all available docker commands

$ docker info
---
-shows system-wide information on docker, among other info

- total number of created containers, which of them are Running/Paused/Stopped
- number of locally available images
- storage driver, i.e. aufs
- root directory, i.e. /var/lib/docker/aufs
- Network, i.e. host null overlay bridge
- Docker Root Dir, i.e. /var/lib/docker


Working with Images:
+++

As we have discussed at length, the key to start working with any docker container is using images. There are many freely available images shared across docker image index and the CLI allows simple access to query the image repository and to download new ones.

When you are ready, you can also share your image there as well. See the section on "push" further down for details.


$ docker search ubuntu
---
-searching for a docker image, in the above example, the image we are searching for is named ubuntu

-this search will provide you a list of all available images matching the query 'ubuntu'


$ docker pull ubuntu
---
-either when you are building / creating a container or before you do, you will need to have an image present at the host machine where the containers will exist. In order to download images (perhaps following "search") you can execute pull to get one


$ docker images
---
-all the images on your system, including the ones you have created by committing (see below for details), can be listed using "images"


$ docker commit 8dbd9e392a96 my_img
---
-as you work with a container and continue to perform actions on it (e.g. download and install software, configure files etc.), to have it keep its state, you need to "commit" 

-committing makes sure that everything continues from where they left next time you use one (i.e. an image)


$ docker push my_username/my_first_image
---
-although it is a bit early at this moment - in our article, when you have created your own container which you would like to share with the rest of the world, you can use push to have your image listed in the index where everybody can download and use

-please remember to "commit" all your changes

-note: You need to sign-up at index.docker.io to push images to docker index


Working with Containers:
+++

When you "run" any process using an image, in return, you will have a container. When the process is not actively running, this container will be a non-running container. Nonetheless, all of them will reside on your system until you remove them via rm command.

$ docker ps
---
-list all running containers

$ docker ps -a
---
-list of both running and non-running ones

$ docker ps -l
---
-list the last run container
-shows only the latest created container, include non-running ones


Creating a New Container:
+++

It is currently not possible to create a container without running anything (i.e. commands). To create a new container, you need to use a base image and specify a command to run.

$ docker run my_img echo "hello"
---
-usage: docker run [image name] [command to run]

$ docker run --name my_cont_1 my_img echo "hello"
---
-to name a container instead of having long IDs

-usage: sudo docker run -name [name] [image name] [command to run]

Running a container:
+++

$ docker run c629b7d70666
---
-usage: docker run [container ID]
-when you create a container and it stops (either due to its process ending or you stopping it explicitly), you can use 'run' to get the container working again with the same command used to create it.

Stopping a container:
+++

$ docker stop c629b7d70666
---
-usage: docker stop [container ID]
-to stop a container's process from running

Saving (committing) a container:
+++

If you would like to save the progress and changes you made with a container, you can use "commit" as explained above to save it as an image.

    This command turns your container to an image.

Remember that with docker, commits are cheap. Do not hesitate to use them to create images to save your progress with a container or to roll back when you need (e.g. like snapshots in time).

Removing / Deleting a container
+++

$ docker rm c629b7d70666
---
-usage: docker rm [container ID]

-using the ID of a container, you can delete one with rm


Title: docker-explained-using-dockerfiles-to-automate-building-of-images
===
Source: https://www.digitalocean.com/community/tutorials/docker-explained-using-dockerfiles-to-automate-building-of-images

Added: Thu Nov  3 13:39:08 CET 2016
Created: PostedDecember 13, 2013

Introduction
+++

Docker containers are created by using [base] images. An image can be basic, with nothing but the operating-system fundamentals, or it can consist of a sophisticated pre-built application stack ready for launch.

When building your images with docker, each action taken (i.e. a command executed such as apt-get install) forms a new layer on top of the previous one. These base images then can be used to create new containers.

In this DigitalOcean article, we will see about automating this process as much as possible, as well as demonstrate the best practices and methods to make most of docker and containers via Dockerfiles: scripts to build containers, step-by-step, layer-by-layer, automatically from a source (base) image.


Docker in Brief
+++

The docker project offers higher-level tools which work together, built on top of some Linux kernel features. The goal is to help developers and system administrators port applications. - with all of their dependencies conjointly - and get them running across systems and machines headache free.

Docker achieves this by creating safe, LXC (i.e. Linux Containers) based environments for applications called “docker containers”. These containers are created using docker images, which can be built either by executing commands manually or automatically through Dockerfiles.


Dockerfiles
+++

Each Dockerfile is a script, composed of various commands (instructions) and arguments listed successively to automatically perform actions on a base image in order to create (or form) a new one. They are used for organizing things and greatly help with deployments by simplifying the process start-to-finish.

Dockerfiles begin with defining an image FROM which the build process starts. Followed by various other methods, commands and arguments (or conditions), in return, provide a new image which is to be used for creating docker containers.

They can be used by providing a Dockerfile's content - in various ways - to the docker daemon to build an image.


Dockerfile Syntax Example
+++

Dockerfile syntax consists of two kind of main line blocks: comments and commands + arguments.

# Line blocks used for commenting
command argument argument ..

A Simple Example

        # Print "Hello docker!"
        RUN echo "Hello docker!"

Dockerfiles use simple, clean, and clear syntax which makes them strikingly easy to create and use. They are designed to be self explanatory, especially because they allow commenting just like a good and properly written application source-code. 


Dockerfile Commands/Instructions
+++

Currently there are about a dozen different set of commands which Dockerfiles can contain to have docker build an image. In this section, we will go over all of them, individually, before working on a Dockerfile example.

Note. As explained in the previous section (Dockerfile Syntax), all these commands are to be listed (i.e. written) successively, inside a single plain text file (i.e. Dockerfile), in the order you would like them performed (i.e. executed) by the docker daemon to build an image. However, some of these commands (e.g. MAINTAINER) can be placed anywhere you seem fit (but always after FROM command), as they do not constitute of any execution but rather value of a definition (i.e. just some additional information).


add
---

The ADD command gets two arguments: a source and a destination. It basically copies the files from the source on the host into the container's own filesystem at the set destination. If, however, the source is a URL (e.g. http://github.com/user/file/), then the contents of the URL are downloaded and placed at the destination.

Example

        # Usage, ADD [source directory or URL] [destination directory]
        ADD /my_app_folder /my_app_folder


cmd
---

The command CMD, similarly to RUN, can be used for executing a specific command. However, unlike RUN it is not executed during build, #but when a container is instantiated# using the image being built. Therefore, it should be considered as an initial, default command that gets executed (i.e. run) with the creation of containers based on the image.

To clarify. an example for CMD would be running an application upon creation of a container which is already installed using RUN (e.g. RUN apt-get install …) inside the image. This default application execution command that is set with CMD becomes the default and replaces any command which is passed during the creation.

Example

        # Usage,  CMD application "argument", "argument", ..
        CMD "echo" "Hello docker!"


entrypoint
---

ENTRYPOINT argument sets the concrete default application that is used every time a container is instantiated/created using the image. For example, if you have installed a specific application inside an image and you will use this image to only run that application, you can state it with ENTRYPOINT and whenever a container is created from that image, your application will be the target.

If you couple ENTRYPOINT with CMD, you can remove "application" from CMD and just leave "arguments" which will be passed to the ENTRYPOINT.

Example:

        # Usage: ENTRYPOINT application "argument", "argument", ..
        # Remember: arguments are optional. They can be provided by CMD
        #           or during the creation of a container. 
        ENTRYPOINT echo

        # Usage example with CMD:
        # Arguments set with CMD can be overridden during *run*
        CMD "Hello docker!"
        ENTRYPOINT echo  


env
---

The ENV command is used to set the environment variables (one or more). These variables consist of “key value” pairs which can be accessed within the container by scripts and applications alike. This functionality of docker offers an enormous amount of flexibility for running programs.

Example:

        #Usage: ENV key value
        ENV SERVER_WORKS 4


expose
---

The EXPOSE command is used to associate a specified port to enable networking between the running process inside the container and the outside world (i.e. the host).

Example:

        #Usage: EXPOSE [port]
        EXPOSE 8080


from
---

FROM directive is probably the most crucial amongst all others for Dockerfiles. It defines the base image to use to start the build process. It can be any image, including the ones you have created previously. If a FROM image is not found on the host, docker will try to find it (and download) from the docker image index. It needs to be the first command declared inside a Dockerfile.

Example:

        # Usage: FROM [image name]
        FROM ubuntu


maintainer
---

One of the commands that can be set anywhere in the file - although it would be better if it was declared on top - is MAINTAINER. This non-executing command declares the author, hence setting the author field of the images. It should come nonetheless after FROM.

Example:

        # Usage: MAINTAINER [name]
        MAINTAINER authors_name


run
---

The RUN command is the central executing directive for Dockerfiles. It takes a command as its argument and runs it to form/create the image. Unlike CMD, #it actually is used to build the image# (forming another layer on top of the previous one which is committed).

Example:

        # Usage: RUN [command]
        RUN aptitude install -y riak


user
---

The USER directive is used to set the UID (or username) which is to run the container based on the image being built.

Example:

        # Usage: USER [UID]
        USER 751


volume
---

The VOLUME command is used to enable access from your container to a directory on the host machine (i.e. mounting it).

Example:

        # Usage: VOLUME ["/dir_1", "/dir_2" ..]
        VOLUME ["/my_files"]


workdir
---

The WORKDIR directive is used to set where the command defined with CMD is to be executed.

Example:

        # Usage: WORKDIR /path
        WORKDIR ~/


How to Use Dockerfiles
+++

Using the Dockerfiles is as simple as having the docker daemon run one. The output after executing the script will be the ID of the new docker image.

Usage:

# Build an image using the Dockerfile at current location

# Example: docker build -t [name] .

$ docker build -t my_mongodb .    
---


Dockerfile Example: Creating an Image to Install MongoDB
+++

In this final section for Dockerfiles, we will create a Dockerfile document and populate it step-by-step with the end result of having a Dockerfile, which can be used to create a docker image to run MongoDB containers.

Creating the Empty Dockerfile
---

Using the nano text editor, let's start editing our Dockerfile.

nano Dockerfile


Defining Our File and Its Purpose
---

Albeit optional, it is always a good practice to let yourself and everybody figure out (when necessary) what this file is and what it is intended to do. For this, we will begin our Dockerfile with fancy comments (i#) to describe it - and have it like cool kids.


        ############################################################
        # Dockerfile to build MongoDB container images
        # Based on Ubuntu
        ############################################################


Setting The Base Image to Use
---

        # Set the base image to Ubuntu
        FROM ubuntu

Defining The Maintainer (Author)
---

        # File Author / Maintainer
        MAINTAINER Example McAuthor


Updating The Application Repository List
---

Note: This step is not necessary, given that we are not using the repository right afterwards. However, it can be considered good practice.


        # Update the repository sources list
        RUN apt-get update


Setting Arguments and Commands for Downloading MongoDB
---


        ################## BEGIN INSTALLATION ######################
        # Install MongoDB Following the Instructions at MongoDB Docs
        # Ref: http://docs.mongodb.org/manual/tutorial/install-mongodb-on-ubuntu/

        # Add the package verification key
        RUN apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10

        # Add MongoDB to the repository sources list

        RUN echo 'deb http://downloads-distro.mongodb.org/repo/ubuntu-upstart dist 10gen' | tee /etc/apt/sources.list.d/mongodb.list

        # Update the repository sources list once more

        RUN apt-get update

        # Install MongoDB package (.deb)
        RUN apt-get install -y mongodb-10gen

        # Create the default data directory
        RUN mkdir -p /data/db

        ##################### INSTALLATION END #####################

MY NOTE: in the comments section it was said it's better practice to avoid using separate RUN directives where possible as each RUN command adds an additional layer to the image, creating unnecessary bloat. Later below in this text I have added a modified Dockerfile which follows this advice.

QUOTE from comments:
February 17, 2015

One thing to note, each time you use the RUN command, it makes a layer in your image which may bloat your total image tree (if you run docker images --tree in the terminal to view the tree structure).
MY NOTE (there is no --tree option to docker images, so presumably the below command does the same job): 
$ docker images --all

If you don't want all those layers, use a single RUN command. A great example of this is actually the Dockerfile for the ubuntu image.
https://github.com/tianon/docker-brew-ubuntu-core/blob/a9da4b3cd8977c2aacafe5d9d0056cbb360f2d1c/vivid/Dockerfile

June 29, 2016

This should be mentioned in the section on the RUN command, since inadvertently adding layers is not best practice or a good habit for a beginner to build. Nice info.



Setting The Default Port For MongoDB
---

        # Expose the default port
        EXPOSE 27017

        # Default port to execute the entrypoint (MongoDB)
        CMD ["--port 27017"]

        # Set default container command
        ENTRYPOINT usr/bin/mongod

The resulting Dockerfile 
---

https://github.com/triogh/ubuntu_server/blob/master/docs/example_docker/mongodb/Dockerfile

Has some of my additional modifications related to RUN directives, and also some updates necessary for the Dockerfile to work in the present time (2016), since the tutorial was created in 2013. 



        ###################################################
        # Dockerfile to build MongoDB container images
        # Based on Ubuntu
        ###################################################

        # Set the base image to Ubuntu
        FROM ubuntu

        # File Author / Maintainer
        MAINTAINER Example McAuthor

        # Update the repository sources list
        RUN apt-get update

        ##################### BEGIN INSTALLATION ####################

        # Install MongoDB Following the Instructions at MongoDB Docs
        # Ref: http://docs.mongodb.org/manual/tutorial/install-mongodb-on-ubuntu/

        # Add the package verification key
        RUN apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv EA312927

        # Add MongoDB to the repository sources list
        RUN echo "deb http://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.2 multiverse" | tee /etc/apt/sources.list.d/mongodb-org-3.2.list

        # Update the repository sources list once more
        RUN apt-get update

        # Install MongoDB package (.deb)
        RUN apt-get install -y mongodb-org

        # Create the default data directory
        RUN mkdir -p /data/db

        # ubuntu 16.04 specific
        # https://docs.mongodb.com/manual/tutorial/install-mongodb-on-ubuntu/

        RUN echo "[Unit]" | tee -a /lib/systemd/system/mongod.service \
            && echo "Description=High-performance, schema-free document-oriented database" | tee -a /lib/systemd/system/mongod.service \
            && echo "After=network.target" | tee -a /lib/systemd/system/mongod.service \
            && echo "Documentation=https://docs.mongodb.org/manual" | tee -a /lib/systemd/system/mongod.service \
            && echo "" | tee -a /lib/systemd/system/mongod.service \
            && echo "[Service]" | tee -a /lib/systemd/system/mongod.service \
            && echo "User=mongodb" | tee -a /lib/systemd/system/mongod.service \
            && echo "Group=mongodb" | tee -a /lib/systemd/system/mongod.service \
            && echo "ExecStart=/usr/bin/mongod --quiet --config /etc/mongod.conf" | tee -a /lib/systemd/system/mongod.service \
            && echo "" | tee -a /lib/systemd/system/mongod.service \
            && echo "[Install]" | tee -a /lib/systemd/system/mongod.service \
            && echo "WantedBy=multi-user.target" | tee -a /lib/systemd/system/mongod.service

        ##################### END INSTALLATION ####################

        # Expose the default port
        EXPOSE 27017

        # Default port to execute the entrypoint (MongoDB)

        CMD ["--port 27017"]

        # Set default container command
        ENTRYPOINT /usr/bin/mongod


Building Our First Image
+++

Using the explanations from before, we are ready to create our first MongoDB image with docker!

$ docker build -t my_mongodb .
---

Note: The -t [name] flag here is used to tag the image. To learn more about what else you can do during build, run sudo docker build --help.


Running A MongoDB Instance
+++

Using the image we have build, we can now proceed to the final step: creating a container running a MongoDB instance inside, using a name of our choice (if desired with -name [name]).

$ docker run --name my_first_mdb_instance -i -t my_mongodb
---

Note: If a name is not set, we will need to deal with complex, alphanumeric IDs which can be obtained by listing all the containers using sudo docker ps -l.

Note: To detach yourself from the container, use the escape sequence CTRL+P followed by CTRL+Q.


Title: docker-explained-how-to-containerize-and-use-nginx-as-a-proxy
===
Source: https://www.digitalocean.com/community/tutorials/docker-explained-how-to-containerize-and-use-nginx-as-a-proxy

Added: Sat Nov  5 18:29:31 CET 2016
Created: PostedDecember 16, 2013

Introduction
+++
In this DigitalOcean article, we will learn about quickly setting up docker, creating a docker container from a base image, and building it to run Nginx layer by layer. Afterwards, following our steps from the beginning, we will create a Dockerfile to automate this entire process. In the end, using this Nginx docker image, you will be able to create self-contained sandboxes running Nginx, which can be used to serve your "dockerised" applications.

Docker in Brief
+++
The docker project offers higher-level tools, working together, which are built on top of some Linux kernel features. The goal is to help developers and system administrators port applications - with all of their dependencies conjointly - and get them running across systems and machines - headache free.

Docker achieves this by creating safe, LXC (i.e. Linux Containers) based environments for applications called “docker containers”. These containers are created using docker images, which can be built either by executing commands manually or automatically through Dockerfiles.


Nginx in Brief
+++
Nginx is a very high performant web server / (reverse)-proxy). It has reached its popularity due to being light weight, relatively easy to work with, and easy to extend (with add-ons / plug-ins). Thanks to its architecture, it is capable of handling a lot of requests (virtually unlimited), which - depending on your application or website load - could be really hard to tackle using older alternatives. It can be considered the tool to choose for serving static files such as images, scripts or style-sheets.

Building a Docker Container With Nginx Installed
+++

Note: Although after following this section we will have a running docker container with Nginx installed, it is definitely not the recommended method due to its complexity. However, it is here to offer you a chance to learn how to work with a live container and get familiarized with the commands we will need to define later to automate the process. To create a docker image with Nginx installed in a much better way, see the next section: Creating a Dockerfile to Automatically Build Nginx Image.

Creating a Base Docker Container From Ubuntu
+++

Using docker's RUN command, we will begin with creating a new container based on the Ubuntu image. We are going to attach a terminal to it using the “-t” flag.

$ docker run -i -t -p 80:80 ubuntu /bin/bash
---

Note: After executing this command, docker might need to pull the Ubuntu image before creating a new container for you.

Remember: You will be attached to the container you create. In order to detach yourself and go back to your main terminal access point, run the escape sequence: CTRL+P followed by CTRL+Q. Being attached to a docker container is like being connected to a new droplet from inside another.

To attach yourself back to this container:

    - List all running containers using sudo docker ps
    - Find its ID
    - Use "$ docker attach [id]" to attach back to its terminal

Important: Please do not forget that since we are in a container, all the following commands will be executed there, without affecting the host.

Preparing the Base Container for Nginx Installation
+++
Update the list with the newly added source.

apt-get update


Before we proceed to install Nginx, there are some tools we should have installed such as nano - just in case.

apt-get install -y nano wget dialog net-tools    



Installing Nginx
+++
Thanks to having it available in the repository, we can simply use apt-get to download and install nginx.

apt-get install -y nginx


Configuring Nginx
+++

Using the text editor nano, which we have installed in the previous step, let's create a sample Nginx configuration to proxy connections to application servers.

# Delete the default configuration
rm -v /etc/nginx/nginx.conf

# Create a blank one using nano text editor
nano /etc/nginx/nginx.conf

First, on top of the file, a line must be added to not to have Nginx spawn its processes and then quit.

The reason we cannot allow this to happen is because docker depends on a single process to run (which can even be a process manager nonetheless) and when that process stops (i.e. quitting after spawning workers), the container stops.

Start with the following as the first line of the nginx.conf:

daemon off;
---

We will use a simple sample configuration to have Nginx run as a reverse proxy. Copy-and-paste the following after the daemon off; instruction.



        worker_processes 1;
        events { worker_connections 1024; }

        http {

            sendfile on;

            gzip              on;
            gzip_http_version 1.0;
            gzip_proxied      any;
            gzip_min_length   500;
            gzip_disable      "MSIE [1-6]\.";
            gzip_types        text/plain text/xml text/css
                              text/comma-separated-values
                              text/javascript
                              application/x-javascript
                              application/atom+xml;

            # List of application servers
            upstream app_servers {

                server 127.0.0.1:8080;

            }

            # Configuration for the server
            server {

                # Running port
                listen 80;

                # Proxying the connections connections
                location / {

                    proxy_pass         http://app_servers;
                    proxy_redirect     off;
                    proxy_set_header   Host $host;
                    proxy_set_header   X-Real-IP $remote_addr;
                    proxy_set_header   X-Forwarded-For $proxy_add_x_forwarded_for;
                    proxy_set_header   X-Forwarded-Host $server_name;

                }
            }
        }




Save and exit pressing CTRL+X and confirming with Y.  
To run Nginx, you can execute the following:

service nginx start

And that's it! We now have Nginx running in a docker container, accessible from the outside world on port 80 as we set using the -p 80:80 flag.

Remember: This Nginx file, albeit configured correctly, will not do anything since there are currently no application servers running on the server. Instead of this one, you can copy and use another example which simply works as a forward proxy for testing HTTP headers until you have your application server(s) installed and working.


Creating the Dockerfile to Automatically Build the Image
+++

As we have mentioned in the previous step, it is certainly not the recommended way to create containers this way for scalable production. The right way to do can be considered as using Dockerfiles to automate the build process in a structured way.

After having gone through the necessary commands for downloading and installing Nginx inside a container, we can use the same knowledge to compose a Dockerfile that docker can use to build an image, which then can be used to run Nginx instances easily.

Before we start working on the Dockerfile, let's quickly go over the basics.


Dockerfile Basics
+++

Dockerfiles are scripts containing commands declared successively which are to be executed in that order by docker to automatically create a new docker image. They help greatly with deployments.

These files always begin with defining an base image using the FROM command. From there on, the build process starts and each following action taken forms the final image which will be committed on the host.

Usage:

        # Build an image using the Dockerfile at current location
        # Tag the final image with [name] (e.g. *nginx*)
        # Example: sudo docker build -t [name] .
        $ docker build -t nginx_img . 

Dockerfile Commands Overview
+++

    - ADD: Copy a file from the host into the container
    - CMD: Set default commands to be executed, or passed to the ENTRYPOINT
    - ENTRYPOINT: Set the default entrypoint application inside the container
    - ENV: Set environment variable (e.g. key = value)
    - EXPOSE: Expose a port to outside
    - FROM: Set the base image to use
    - MAINTAINER: Set the author / owner data of the Dockerfile
    - RUN: Run a command and commit the ending result (container) image
    - USER: Set the user to run the containers from the image
    - VOLUME: Mount a directory from the host to the container
    - WORKDIR: Set the directory for the directives of CMD to be executed


