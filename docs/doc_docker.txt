Docker Documentation
***

Title: how-to-use-the-digitalocean-docker-application
===
Source: https://www.digitalocean.com/community/tutorials/how-to-use-the-digitalocean-docker-application

Added: Wed Nov  2 09:53:53 CET 2016
Created: PostedSeptember 20, 2013


$ docker run ubuntu echo "Hello World"
+++
-docker will search for the 'ubuntu' image on your local machine, if it doesn't find it it will pull the image from the 'Docker Central Registry'

-docker will run a container from an image named "ubuntu" (which contains the whole "ubuntu" root file system ~180MB)

-docker will execute 'echo "Hello World"' in the shell (in the container) and exit


$ docker images
+++
-list docker images which have been downloaded/cached or created on your local machine


$ docker ps -a
+++
-lists all containers, even those which have exited (are not running any more)

-those which have exited are visible because the writable file system layer used by the temporary container (for example the one mentioned above, which echo-ed "Hello World" and exited) is still around


$ docker rm
+++
-remove a container


$ docker rmi
+++
-remove an image


$ docker version
+++
-shows currently installed docker client/server version



Title: how-to-install-and-use-docker-getting-started
===
Source: https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-getting-started

Added: Wed Nov  2 10:39:28 CET 2016
Created: PostedDecember 11, 2013


Docker is here to offer you an efficient, speedy way to port applications across systems and machines. It is light and lean, allowing you to quickly contain applications and run them within their own secure environments (via Linux Containers: LXC). 

Whether it be from your development machine to a remote server for production, or packaging everything for use elsewhere, it is always a challenge when it comes to porting your application stack together with its dependencies and getting it to run without hiccups. In fact, the challenge is immense and solutions so far have not really proved successful for the masses.

In a nutshell, docker as a project offers you the complete set of higher-level tools to carry everything that forms an application across systems and machines - virtual or physical - and brings along loads more of great benefits with it.

Docker achieves its robust application (and therefore, process and resource) containment via Linux Containers (e.g. namespaces and other kernel features). Its further capabilities come from a project's own parts and components, which extract all the complexity of working with lower-level linux tools/APIs used for system and application management with regards to securely containing processes.

Main Docker Parts:
+++

- docker daemon: used to manage docker (LXC) containers on the host it runs
- docker CLI: used to command and communicate with the docker daemon
- docker image index: a repository (public or private) for docker images

Main Docker Elements:
+++

- docker containers: directories containing everything-your-application
- docker images: snapshots of containers or base OS (e.g. Ubuntu) images
- Dockerfiles: scripts automating the building process of images


Docker Containers:
+++

The entire procedure of porting applications using docker relies solely on the shipment of containers.

Docker containers are basically directories which can be packed (e.g. tar-archived) like any other, then shared and run across various different machines and platforms (hosts). The only dependency is having the hosts tuned to run the containers (i.e. have docker installed). Containment here is obtained via Linux Containers (LXC).

Linux Containers can be defined as a combination various kernel-level features (i.e. things that Linux-kernel can do) which allow management of applications (and resources they use) contained within their own environment. By making use of certain features (e.g. namespaces, chroots, cgroups and SELinux profiles), the LXC contains application processes and helps with their management through limiting resources, not allowing reach beyond their own file-system (access to the parent's namespace) etc.

Docker with its containers makes use of LXC, however, also brings along much more.

Being based and depending on LXC, from a technical aspect, these containers are like a directory (but a shaped and formatted one). This allows portability and gradual builds of containers.

Each container is layered like an onion and each action taken within a container consists of putting another block (which actually translates to a simple change within the file system) on top of the previous one. And various tools and configurations make this set-up work in a harmonious way altogether (e.g. union file-system/aufs).

What this way of having containers allows is the extreme benefit of easily launching and creating new containers and images, which are thus kept lightweight (thanks to gradual and layered way they are built). Since everything is based on the file-system, taking snapshots and performing roll-backs in time are cheap (i.e. very easily done / not heavy on resources), much like version control systems (VCS).

Each docker container starts from a docker image which forms the base for other applications and layers to come.


Docker Images:
+++

Docker images constitute the base of docker containers from which everything starts to form. They are very similar to default operating-system disk images which are used to run applications on servers or desktop computers.

Having these images (e.g. Ubuntu base) allow seamless portability across systems. They make a solid, consistent and dependable base with everything that is needed to run the applications. When everything is self-contained and the risk of system-level updates or modifications are eliminated, the container becomes immune to external exposures which could put it out of order - preventing the dependency hell.

As more layers (tools, applications etc.) are added on top of the base, new images can be formed by committing these changes. When a new container gets created from a saved (i.e. committed) image, things continue from where they left off. And the union file system, brings all the layers together as a single entity when you work with a container.

These base images can be explicitly stated when working with the docker CLI to directly create a new container or they might be specified inside a Dockerfile for automated image building.


Dockerfiles
+++

Dockerfiles are scripts containing a successive series of instructions, directions, and commands which are to be executed to form a new docker image. Each command executed translates to a new layer of the onion, forming the end product. They basically replace the process of doing everything manually and repeatedly. When a Dockerfile is finished executing, you end up having formed an image, which then you use to start (i.e. create) a new container.


$ docker
---
-lists all available docker commands

$ docker info
---
-shows system-wide information on docker, among other info

- total number of created containers, which of them are Running/Paused/Stopped
- number of locally available images
- storage driver, i.e. aufs
- root directory, i.e. /var/lib/docker/aufs
- Network, i.e. host null overlay bridge
- Docker Root Dir, i.e. /var/lib/docker


Working with Images:
+++

As we have discussed at length, the key to start working with any docker container is using images. There are many freely available images shared across docker image index and the CLI allows simple access to query the image repository and to download new ones.

When you are ready, you can also share your image there as well. See the section on "push" further down for details.


$ docker search ubuntu
---
-searching for a docker image, in the above example, the image we are searching for is named ubuntu

-this search will provide you a list of all available images matching the query 'ubuntu'


$ docker pull ubuntu
---
-either when you are building / creating a container or before you do, you will need to have an image present at the host machine where the containers will exist. In order to download images (perhaps following "search") you can execute pull to get one


$ docker images
---
-all the images on your system, including the ones you have created by committing (see below for details), can be listed using "images"


$ docker commit 8dbd9e392a96 my_img
---
-as you work with a container and continue to perform actions on it (e.g. download and install software, configure files etc.), to have it keep its state, you need to "commit" 

-committing makes sure that everything continues from where they left next time you use one (i.e. an image)


$ docker push my_username/my_first_image
---
-although it is a bit early at this moment - in our article, when you have created your own container which you would like to share with the rest of the world, you can use push to have your image listed in the index where everybody can download and use

-please remember to "commit" all your changes

-note: You need to sign-up at index.docker.io to push images to docker index


Working with Containers:
+++

When you "run" any process using an image, in return, you will have a container. When the process is not actively running, this container will be a non-running container. Nonetheless, all of them will reside on your system until you remove them via rm command.

$ docker ps
---
-list all running containers

$ docker ps -a
---
-list of both running and non-running ones

$ docker ps -l
---
-list the last run container
-shows only the latest created container, include non-running ones


Creating a New Container:
+++

It is currently not possible to create a container without running anything (i.e. commands). To create a new container, you need to use a base image and specify a command to run.

$ docker run my_img echo "hello"
---
-usage: docker run [image name] [command to run]

$ docker run --name my_cont_1 my_img echo "hello"
---
-to name a container instead of having long IDs

-usage: sudo docker run -name [name] [image name] [command to run]

Running a container:
+++

$ docker start c629b7d70666
---
-usage: docker start [container ID]
-when you create a container and it stops (either due to its process ending or you stopping it explicitly), you can use 'start' to get the container working again with the same command used to create it.

MY NOTE: I think "docker run [image]" is used to create a container from an image, while "docker start [container ID]" is used to start an already created container, which was stopped for some reason. That is why I changed 'run' to 'start' in the command example given above, and in it's description.

Stopping a container:
+++

$ docker stop c629b7d70666
---
-usage: docker stop [container ID]
-to stop a container's process from running

Saving (committing) a container:
+++

If you would like to save the progress and changes you made with a container, you can use "commit" as explained above to save it as an image.

    This command turns your container to an image.

Remember that with docker, commits are cheap. Do not hesitate to use them to create images to save your progress with a container or to roll back when you need (e.g. like snapshots in time).

Removing / Deleting a container
+++

$ docker rm c629b7d70666
---
-usage: docker rm [container ID]

-using the ID of a container, you can delete one with rm


Title: docker-explained-using-dockerfiles-to-automate-building-of-images
===
Source: https://www.digitalocean.com/community/tutorials/docker-explained-using-dockerfiles-to-automate-building-of-images

Added: Thu Nov  3 13:39:08 CET 2016
Created: PostedDecember 13, 2013

Introduction
+++

Docker containers are created by using [base] images. An image can be basic, with nothing but the operating-system fundamentals, or it can consist of a sophisticated pre-built application stack ready for launch.

When building your images with docker, each action taken (i.e. a command executed such as apt-get install) forms a new layer on top of the previous one. These base images then can be used to create new containers.

In this DigitalOcean article, we will see about automating this process as much as possible, as well as demonstrate the best practices and methods to make most of docker and containers via Dockerfiles: scripts to build containers, step-by-step, layer-by-layer, automatically from a source (base) image.


Docker in Brief
+++

The docker project offers higher-level tools which work together, built on top of some Linux kernel features. The goal is to help developers and system administrators port applications. - with all of their dependencies conjointly - and get them running across systems and machines headache free.

Docker achieves this by creating safe, LXC (i.e. Linux Containers) based environments for applications called “docker containers”. These containers are created using docker images, which can be built either by executing commands manually or automatically through Dockerfiles.


Dockerfiles
+++

Each Dockerfile is a script, composed of various commands (instructions) and arguments listed successively to automatically perform actions on a base image in order to create (or form) a new one. They are used for organizing things and greatly help with deployments by simplifying the process start-to-finish.

Dockerfiles begin with defining an image FROM which the build process starts. Followed by various other methods, commands and arguments (or conditions), in return, provide a new image which is to be used for creating docker containers.

They can be used by providing a Dockerfile's content - in various ways - to the docker daemon to build an image.


Dockerfile Syntax Example
+++

Dockerfile syntax consists of two kind of main line blocks: comments and commands + arguments.

# Line blocks used for commenting
command argument argument ..

A Simple Example

        # Print "Hello docker!"
        RUN echo "Hello docker!"

Dockerfiles use simple, clean, and clear syntax which makes them strikingly easy to create and use. They are designed to be self explanatory, especially because they allow commenting just like a good and properly written application source-code. 


Dockerfile Commands/Instructions
+++

Currently there are about a dozen different set of commands which Dockerfiles can contain to have docker build an image. In this section, we will go over all of them, individually, before working on a Dockerfile example.

Note. As explained in the previous section (Dockerfile Syntax), all these commands are to be listed (i.e. written) successively, inside a single plain text file (i.e. Dockerfile), in the order you would like them performed (i.e. executed) by the docker daemon to build an image. However, some of these commands (e.g. MAINTAINER) can be placed anywhere you seem fit (but always after FROM command), as they do not constitute of any execution but rather value of a definition (i.e. just some additional information).


add
---

The ADD command gets two arguments: a source and a destination. It basically copies the files from the source on the host into the container's own filesystem at the set destination. If, however, the source is a URL (e.g. http://github.com/user/file/), then the contents of the URL are downloaded and placed at the destination.

Example

        # Usage, ADD [source directory or URL] [destination directory]
        ADD /my_app_folder /my_app_folder


cmd
---

The command CMD, similarly to RUN, can be used for executing a specific command. However, unlike RUN it is not executed during build, #but when a container is instantiated# using the image being built. Therefore, it should be considered as an initial, default command that gets executed (i.e. run) with the creation of containers based on the image.

To clarify. an example for CMD would be running an application upon creation of a container which is already installed using RUN (e.g. RUN apt-get install …) inside the image. This default application execution command that is set with CMD becomes the default and replaces any command which is passed during the creation.

Example

        # Usage,  CMD application "argument", "argument", ..
        CMD "echo" "Hello docker!"


entrypoint
---

ENTRYPOINT argument sets the concrete default application that is used every time a container is instantiated/created using the image. For example, if you have installed a specific application inside an image and you will use this image to only run that application, you can state it with ENTRYPOINT and whenever a container is created from that image, your application will be the target.

If you couple ENTRYPOINT with CMD, you can remove "application" from CMD and just leave "arguments" which will be passed to the ENTRYPOINT.

Example:

        # Usage: ENTRYPOINT application "argument", "argument", ..
        # Remember: arguments are optional. They can be provided by CMD
        #           or during the creation of a container. 
        ENTRYPOINT echo

        # Usage example with CMD:
        # Arguments set with CMD can be overridden during *run*
        CMD "Hello docker!"
        ENTRYPOINT echo  


env
---

The ENV command is used to set the environment variables (one or more). These variables consist of “key value” pairs which can be accessed within the container by scripts and applications alike. This functionality of docker offers an enormous amount of flexibility for running programs.

Example:

        #Usage: ENV key value
        ENV SERVER_WORKS 4


expose
---

The EXPOSE command is used to associate a specified port to enable networking between the running process inside the container and the outside world (i.e. the host).

Example:

        #Usage: EXPOSE [port]
        EXPOSE 8080


from
---

FROM directive is probably the most crucial amongst all others for Dockerfiles. It defines the base image to use to start the build process. It can be any image, including the ones you have created previously. If a FROM image is not found on the host, docker will try to find it (and download) from the docker image index. It needs to be the first command declared inside a Dockerfile.

Example:

        # Usage: FROM [image name]
        FROM ubuntu


maintainer
---

One of the commands that can be set anywhere in the file - although it would be better if it was declared on top - is MAINTAINER. This non-executing command declares the author, hence setting the author field of the images. It should come nonetheless after FROM.

Example:

        # Usage: MAINTAINER [name]
        MAINTAINER authors_name


run
---

The RUN command is the central executing directive for Dockerfiles. It takes a command as its argument and runs it to form/create the image. Unlike CMD, #it actually is used to build the image# (forming another layer on top of the previous one which is committed).

Example:

        # Usage: RUN [command]
        RUN aptitude install -y riak


user
---

The USER directive is used to set the UID (or username) which is to run the container based on the image being built.

Example:

        # Usage: USER [UID]
        USER 751


volume
---

The VOLUME command is used to enable access from your container to a directory on the host machine (i.e. mounting it).

Example:

        # Usage: VOLUME ["/dir_1", "/dir_2" ..]
        VOLUME ["/my_files"]


workdir
---

The WORKDIR directive is used to set where the command defined with CMD is to be executed.

Example:

        # Usage: WORKDIR /path
        WORKDIR ~/


How to Use Dockerfiles
+++

Using the Dockerfiles is as simple as having the docker daemon run one. The output after executing the script will be the ID of the new docker image.

Usage:

# Build an image using the Dockerfile at current location

# Example: docker build -t [name] .

$ docker build -t my_mongodb .    
---


Dockerfile Example: Creating an Image to Install MongoDB
+++

In this final section for Dockerfiles, we will create a Dockerfile document and populate it step-by-step with the end result of having a Dockerfile, which can be used to create a docker image to run MongoDB containers.

Creating the Empty Dockerfile
---

Using the nano text editor, let's start editing our Dockerfile.

nano Dockerfile


Defining Our File and Its Purpose
---

Albeit optional, it is always a good practice to let yourself and everybody figure out (when necessary) what this file is and what it is intended to do. For this, we will begin our Dockerfile with fancy comments (i#) to describe it - and have it like cool kids.


        ############################################################
        # Dockerfile to build MongoDB container images
        # Based on Ubuntu
        ############################################################


Setting The Base Image to Use
---

        # Set the base image to Ubuntu
        FROM ubuntu

Defining The Maintainer (Author)
---

        # File Author / Maintainer
        MAINTAINER Example McAuthor


Updating The Application Repository List
---

Note: This step is not necessary, given that we are not using the repository right afterwards. However, it can be considered good practice.


        # Update the repository sources list
        RUN apt-get update


Setting Arguments and Commands for Downloading MongoDB
---


        ################## BEGIN INSTALLATION ######################
        # Install MongoDB Following the Instructions at MongoDB Docs
        # Ref: http://docs.mongodb.org/manual/tutorial/install-mongodb-on-ubuntu/

        # Add the package verification key
        RUN apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10

        # Add MongoDB to the repository sources list

        RUN echo 'deb http://downloads-distro.mongodb.org/repo/ubuntu-upstart dist 10gen' | tee /etc/apt/sources.list.d/mongodb.list

        # Update the repository sources list once more

        RUN apt-get update

        # Install MongoDB package (.deb)
        RUN apt-get install -y mongodb-10gen

        # Create the default data directory
        RUN mkdir -p /data/db

        ##################### INSTALLATION END #####################

MY NOTE: in the comments section it was said it's better practice to avoid using separate RUN directives where possible as each RUN command adds an additional layer to the image, creating unnecessary bloat. Later below in this text I have added a modified Dockerfile which follows this advice.

QUOTE from comments:
February 17, 2015

One thing to note, each time you use the RUN command, it makes a layer in your image which may bloat your total image tree (if you run docker images --tree in the terminal to view the tree structure).
MY NOTE (there is no --tree option to docker images, so presumably the below command does the same job): 
$ docker images --all

If you don't want all those layers, use a single RUN command. A great example of this is actually the Dockerfile for the ubuntu image.
https://github.com/tianon/docker-brew-ubuntu-core/blob/a9da4b3cd8977c2aacafe5d9d0056cbb360f2d1c/vivid/Dockerfile

June 29, 2016

This should be mentioned in the section on the RUN command, since inadvertently adding layers is not best practice or a good habit for a beginner to build. Nice info.



Setting The Default Port For MongoDB
---

        # Expose the default port
        EXPOSE 27017

        # Default port to execute the entrypoint (MongoDB)
        CMD ["--port 27017"]

        # Set default container command
        ENTRYPOINT usr/bin/mongod

The resulting Dockerfile 
---

https://github.com/triogh/ubuntu_server/blob/master/docs/example_docker/mongodb/Dockerfile

Has some of my additional modifications related to RUN directives, and also some updates necessary for the Dockerfile to work in the present time (2016), since the tutorial was created in 2013. 



        ###################################################
        # Dockerfile to build MongoDB container images
        # Based on Ubuntu
        ###################################################

        # Set the base image to Ubuntu
        FROM ubuntu

        # File Author / Maintainer
        MAINTAINER Example McAuthor

        # Update the repository sources list
        RUN apt-get update

        ##################### BEGIN INSTALLATION ####################

        # Install MongoDB Following the Instructions at MongoDB Docs
        # Ref: http://docs.mongodb.org/manual/tutorial/install-mongodb-on-ubuntu/

        # Add the package verification key
        RUN apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv EA312927

        # Add MongoDB to the repository sources list
        RUN echo "deb http://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.2 multiverse" | tee /etc/apt/sources.list.d/mongodb-org-3.2.list

        # Update the repository sources list once more
        RUN apt-get update

        # Install MongoDB package (.deb)
        RUN apt-get install -y mongodb-org

        # Create the default data directory
        RUN mkdir -p /data/db

        # ubuntu 16.04 specific
        # https://docs.mongodb.com/manual/tutorial/install-mongodb-on-ubuntu/

        RUN echo "[Unit]" | tee -a /lib/systemd/system/mongod.service \
            && echo "Description=High-performance, schema-free document-oriented database" | tee -a /lib/systemd/system/mongod.service \
            && echo "After=network.target" | tee -a /lib/systemd/system/mongod.service \
            && echo "Documentation=https://docs.mongodb.org/manual" | tee -a /lib/systemd/system/mongod.service \
            && echo "" | tee -a /lib/systemd/system/mongod.service \
            && echo "[Service]" | tee -a /lib/systemd/system/mongod.service \
            && echo "User=mongodb" | tee -a /lib/systemd/system/mongod.service \
            && echo "Group=mongodb" | tee -a /lib/systemd/system/mongod.service \
            && echo "ExecStart=/usr/bin/mongod --quiet --config /etc/mongod.conf" | tee -a /lib/systemd/system/mongod.service \
            && echo "" | tee -a /lib/systemd/system/mongod.service \
            && echo "[Install]" | tee -a /lib/systemd/system/mongod.service \
            && echo "WantedBy=multi-user.target" | tee -a /lib/systemd/system/mongod.service

        ##################### END INSTALLATION ####################

        # Expose the default port
        EXPOSE 27017

        # Default port to execute the entrypoint (MongoDB)

        CMD ["--port 27017"]

        # Set default container command
        ENTRYPOINT /usr/bin/mongod


Building Our First Image
+++

Using the explanations from before, we are ready to create our first MongoDB image with docker!

$ docker build -t my_mongodb .
---

Note: The -t [name] flag here is used to tag the image. To learn more about what else you can do during build, run sudo docker build --help.


Running A MongoDB Instance
+++

Using the image we have build, we can now proceed to the final step: creating a container running a MongoDB instance inside, using a name of our choice (if desired with -name [name]).

$ docker run --name my_first_mdb_instance -i -t my_mongodb
---

Note: If a name is not set, we will need to deal with complex, alphanumeric IDs which can be obtained by listing all the containers using sudo docker ps -l.

Note: To detach yourself from the container, use the escape sequence CTRL+P followed by CTRL+Q.


Title: docker-explained-how-to-containerize-and-use-nginx-as-a-proxy
===
Source: https://www.digitalocean.com/community/tutorials/docker-explained-how-to-containerize-and-use-nginx-as-a-proxy

Added: Sat Nov  5 18:29:31 CET 2016
Created: PostedDecember 16, 2013

Introduction
+++
In this DigitalOcean article, we will learn about quickly setting up docker, creating a docker container from a base image, and building it to run Nginx layer by layer. Afterwards, following our steps from the beginning, we will create a Dockerfile to automate this entire process. In the end, using this Nginx docker image, you will be able to create self-contained sandboxes running Nginx, which can be used to serve your "dockerised" applications.

Docker in Brief
+++
The docker project offers higher-level tools, working together, which are built on top of some Linux kernel features. The goal is to help developers and system administrators port applications - with all of their dependencies conjointly - and get them running across systems and machines - headache free.

Docker achieves this by creating safe, LXC (i.e. Linux Containers) based environments for applications called “docker containers”. These containers are created using docker images, which can be built either by executing commands manually or automatically through Dockerfiles.


Nginx in Brief
+++
Nginx is a very high performant web server / (reverse)-proxy). It has reached its popularity due to being light weight, relatively easy to work with, and easy to extend (with add-ons / plug-ins). Thanks to its architecture, it is capable of handling a lot of requests (virtually unlimited), which - depending on your application or website load - could be really hard to tackle using older alternatives. It can be considered the tool to choose for serving static files such as images, scripts or style-sheets.

Building a Docker Container With Nginx Installed
+++

Note: Although after following this section we will have a running docker container with Nginx installed, it is definitely not the recommended method due to its complexity. However, it is here to offer you a chance to learn how to work with a live container and get familiarized with the commands we will need to define later to automate the process. To create a docker image with Nginx installed in a much better way, see the next section: Creating a Dockerfile to Automatically Build Nginx Image.

Creating a Base Docker Container From Ubuntu
+++

Using docker's RUN command, we will begin with creating a new container based on the Ubuntu image. We are going to attach a terminal to it using the “-t” flag.

$ docker run -i -t -p 80:80 ubuntu /bin/bash
---

Note: After executing this command, docker might need to pull the Ubuntu image before creating a new container for you.

Remember: You will be attached to the container you create. In order to detach yourself and go back to your main terminal access point, run the escape sequence: CTRL+P followed by CTRL+Q. Being attached to a docker container is like being connected to a new droplet from inside another.

To attach yourself back to this container:

    - List all running containers using sudo docker ps
    - Find its ID
    - Use "$ docker attach [id]" to attach back to its terminal

Important: Please do not forget that since we are in a container, all the following commands will be executed there, without affecting the host.

Preparing the Base Container for Nginx Installation
+++
Update the list with the newly added source.

apt-get update


Before we proceed to install Nginx, there are some tools we should have installed such as nano - just in case.

apt-get install -y nano wget dialog net-tools    



Installing Nginx
+++
Thanks to having it available in the repository, we can simply use apt-get to download and install nginx.

apt-get install -y nginx


Configuring Nginx
+++

Using the text editor nano, which we have installed in the previous step, let's create a sample Nginx configuration to proxy connections to application servers.

# Delete the default configuration
rm -v /etc/nginx/nginx.conf

# Create a blank one using nano text editor
nano /etc/nginx/nginx.conf

First, on top of the file, a line must be added to not to have Nginx spawn its processes and then quit.

The reason we cannot allow this to happen is because docker depends on a single process to run (which can even be a process manager nonetheless) and when that process stops (i.e. quitting after spawning workers), the container stops.

Start with the following as the first line of the nginx.conf:

daemon off;
---

We will use a simple sample configuration to have Nginx run as a reverse proxy. Copy-and-paste the following after the daemon off; instruction.



        worker_processes 1;
        events { worker_connections 1024; }

        http {

            sendfile on;

            gzip              on;
            gzip_http_version 1.0;
            gzip_proxied      any;
            gzip_min_length   500;
            gzip_disable      "MSIE [1-6]\.";
            gzip_types        text/plain text/xml text/css
                              text/comma-separated-values
                              text/javascript
                              application/x-javascript
                              application/atom+xml;

            # List of application servers
            upstream app_servers {

                server 127.0.0.1:8080;

            }

            # Configuration for the server
            server {

                # Running port
                listen 80;

                # Proxying the connections connections
                location / {

                    proxy_pass         http://app_servers;
                    proxy_redirect     off;
                    proxy_set_header   Host $host;
                    proxy_set_header   X-Real-IP $remote_addr;
                    proxy_set_header   X-Forwarded-For $proxy_add_x_forwarded_for;
                    proxy_set_header   X-Forwarded-Host $server_name;

                }
            }
        }




Save and exit pressing CTRL+X and confirming with Y.  
To run Nginx, you can execute the following:

service nginx start

And that's it! We now have Nginx running in a docker container, accessible from the outside world on port 80 as we set using the -p 80:80 flag.

Remember: This Nginx file, albeit configured correctly, will not do anything since there are currently no application servers running on the server. Instead of this one, you can copy and use another example which simply works as a forward proxy for testing HTTP headers until you have your application server(s) installed and working.


Creating the Dockerfile to Automatically Build the Image
+++

As we have mentioned in the previous step, it is certainly not the recommended way to create containers this way for scalable production. The right way to do can be considered as using Dockerfiles to automate the build process in a structured way.

After having gone through the necessary commands for downloading and installing Nginx inside a container, we can use the same knowledge to compose a Dockerfile that docker can use to build an image, which then can be used to run Nginx instances easily.

Before we start working on the Dockerfile, let's quickly go over the basics.


Dockerfile Basics
+++

Dockerfiles are scripts containing commands declared successively which are to be executed in that order by docker to automatically create a new docker image. They help greatly with deployments.

These files always begin with defining an base image using the FROM command. From there on, the build process starts and each following action taken forms the final image which will be committed on the host.

Usage:

        # Build an image using the Dockerfile at current location
        # Tag the final image with [name] (e.g. *nginx*)
        # Example: sudo docker build -t [name] .
        $ docker build -t nginx_img . 

Dockerfile Commands Overview
+++

    - ADD: Copy a file from the host into the container
    - CMD: Set default commands to be executed, or passed to the ENTRYPOINT
    - ENTRYPOINT: Set the default entrypoint application inside the container
    - ENV: Set environment variable (e.g. key = value)
    - EXPOSE: Expose a port to outside
    - FROM: Set the base image to use
    - MAINTAINER: Set the author / owner data of the Dockerfile
    - RUN: Run a command and commit the ending result (container) image
    - USER: Set the user to run the containers from the image
    - VOLUME: Mount a directory from the host to the container
    - WORKDIR: Set the directory for the directives of CMD to be executed



Creating the Dockerfile
+++

To create a Dockerfile at the current location using the nano text editor, execute the following command:

		nano Dockerfile

Note: Append all the following lines one after the other to form the Dockerfile to be saved and used for building.
Defining the Fundamentals

Let's begin our Dockerfile by defining the basics (fundamentals) such as the FROM image (i.e. Ubuntu) and the MAINTAINER.


		############################################################
		# Dockerfile to build Nginx Installed Containers
		# Based on Ubuntu
		############################################################

		# Set the base image to Ubuntu
		FROM ubuntu

		# File Author / Maintainer
		MAINTAINER Maintaner Name


Installation Instructions for Nginx
+++

Following our steps from the previous section, let's form the block to have Nginx installed.


		# Install Nginx
		# Update the repository
		RUN apt-get update

		# Install necessary tools
		RUN apt-get install -y nano wget dialog net-tools

		# Download and Install Nginx
		RUN apt-get install -y nginx    


Bootstrapping
+++

After adding the instructions for installing Nginx, let's finish off with configuring Nginx and getting Dockerfile to replace the default configuration file with one we provide during build.


		# Remove the default Nginx configuration file
		RUN rm -v /etc/nginx/nginx.conf

		# Copy a configuration file from the current directory
		ADD nginx.conf /etc/nginx/

		# Append "daemon off;" to the beginning of the configuration
		RUN echo "daemon off;" >> /etc/nginx/nginx.conf

		# Expose ports
		EXPOSE 80

		# Set the default command to execute
		# when creating a new container
		CMD service nginx start


Final Dockerfile
+++

In the end, this is what the Dockerfile should look like:


		############################################################
		# Dockerfile to build Nginx Installed Containers
		# Based on Ubuntu
		############################################################

		# Set the base image to Ubuntu
		FROM ubuntu

		# File Author / Maintainer
		MAINTAINER Maintaner Name

		# Install Nginx

		# Add application repository URL to the default sources
		RUN echo "deb http://archive.ubuntu.com/ubuntu/ raring main universe" >> /etc/apt/sources.list

		# Update the repository
		RUN apt-get update

		# Install necessary tools
		RUN apt-get install -y nano wget dialog net-tools

		# Download and Install Nginx
		RUN apt-get install -y nginx  

		# Remove the default Nginx configuration file
		RUN rm -v /etc/nginx/nginx.conf

		# Copy a configuration file from the current directory
		ADD nginx.conf /etc/nginx/

		# Append "daemon off;" to the beginning of the configuration
		RUN echo "daemon off;" >> /etc/nginx/nginx.conf

		# Expose ports
		EXPOSE 80

		# Set the default command to execute
		# when creating a new container
		CMD service nginx start


MY NOTE:
Below is a Dockerfile, that builds an image with nginx installed and started.

To build an image from the docker file below, you could issue the command below.

$ docker build -t my_nginx_image .
---

To run a container with that image you could issue the command below.

$ docker run -it -p80:80 --name my_nginx_container [image]
---

To run the same container and have a bash shell.

$ docker run -it -p80:80 --name [container_name] [image] /bin/bash
---


		####################################################
		# Dockerfile to build Nginx Installed Containers
		# Based on Ubuntu
		####################################################

		# Set the base image to Ubuntu
		FROM ubuntu

		# File Author / Maintainer
		MAINTAINER User McExample

		##################### Install Nginx ####################

		# Update the repository
		# Install necessary tools
		# Install Nginx
		RUN apt-get update \
			&& apt-get install -y vim wget dialog net-tools \
			&& apt-get install -y nginx \
			&& echo "daemon off;" >> /etc/nginx/nginx.conf


		# Expose ports
		EXPOSE 80

		# Set default command to execute when creating new container
		CMD service nginx start



Using the Dockerfile to Automatically Build Nginx Containers
+++

As we first went over in the "basics" section, Dockerfiles' usage consists of calling them with “docker build” command.

Since we are instructing docker to copy a configuration (i.e. nginx.conf) from the current directory to replace the default one, we need to make sure to have it alongside this Dockerfile before starting the build process.

Note: The above explained procedure of copying in an Nginx configuration allows you great flexibility and saves a lot of time by not dealing with attaching and detaching yourself from containers to create configuration files. Now you can simply use one to directly build and run an image.


Create a sample nginx.conf using the text editor nano:



        nano nginx.conf



And replace its contents to use it as a forward proxy for testing:



		worker_processes 1;
		events { worker_connections 1024; }

		http {

			sendfile on;

			server {

				listen 80;

				location / {
					proxy_pass http://httpstat.us/;
					proxy_set_header  X-Real-IP  $remote_addr;
				}
			}
		}


MY NOTE:
The final Docker file I used succesfully.



		####################################################
		# Dockerfile to build Nginx Installed Containers
		# Based on Ubuntu
		####################################################

		# Set the base image to Ubuntu
		FROM ubuntu

		# File Author / Maintainer
		MAINTAINER User McExample

		##################### Install Nginx ####################

		# Update the repository
		# Install necessary tools
		# Install Nginx
		RUN apt-get update \
			&& apt-get install -y vim wget dialog net-tools \
			&& apt-get install -y nginx \
			&& rm -v /etc/nginx/nginx.conf

		# Copy a configuration file from the current directory
		ADD nginx.conf /etc/nginx


		# Expose ports
		EXPOSE 80

		# Set default command to execute when creating new container
		CMD service nginx start


MY NOTE:
The nginx.conf file I used, same as the one provided by Digital Ocean, except the one i succesfully used has "daemon off;" on top.



		daemon off;
		worker_processes 1;
		events { worker_connections 1024; }

		http {

			sendfile on;

			server {

				listen 80;

				location / {
					proxy_pass http://httpstat.us/;
					proxy_set_header  X-Real-IP  $remote_addr;
				}
			}
		}



This docker image will allow us to port all our progress and quickly create containers running Nginx with a single command.

To start using it, build a new container image with the following:

$docker build -t nginx_img_1 . 
---

And using that image - which we tagged as nginximg1 - we can run a new container:

$ docker run --name nginx_cont_1 -p 80:80 -i -t nginx_img_1
---

Now you can visit the IP address of your droplet, and your Nginx running docker container shall do its job, forwarding you to the HTTP status testing page.

Example:

		# Usage: Visit http://[my droplet's ip]
		http://95.85.10.236/200

		Sample Response:

		200 OK


Title: docker-explained-how-to-create-docker-containers-running-memcached
===
Source: https://www.digitalocean.com/community/tutorials/docker-explained-how-to-create-docker-containers-running-memcached

Added: Sun Nov  6 12:58:18 CET 2016
Created: PostedDecember 16, 2013


Introduction
+++

For the majority of web applications, it is extremely rare to find the CPU being the culprit for the dropped HTTP requests or choking the web server hosting them. It usually is an under-engineered setup with no caching layer involved, eating up all the resources of the backend data store (i.e. your database of choice).

Memcached - which by now should need no introduction - can increase the performance of your application deployment stack greatly without making any amendments to your available resources (enabling you to squeeze every last bit of its juices).

In this DigitalOcean article, especially keeping in mind those who host multiple web applications (e.g. multiple WordPress instances, Python Applications, etc.), we are going to create docker images to quickly start running (on-demand) Memcached containers which can be operated individually. These containers, kept and secured within their own environments, will work with the application being hosted to help them get better and faster.


Memcached in Brief
+++

Memcached is a distributed, open-source data storage engine. It was designed to store certain types of data in RAM (instead of slower rate traditional disks) for very fast retrievals by applications, cutting the amount of time it takes to process requests by reducing the number of queries performed against heavier datasets or APIs such as traditional databases (e.g. MySQL).

By introducing a smart, well-planned, and optimized caching mechanism, it becomes possible to handle a seemingly larger amount of requests and perform more procedures by applications. This is the most important use case of Memcached, as it is with any other caching application or component.

Heavily relied upon and used in production for web sites and various other applications, Memcached has become one of the go-to tools for increasing performance without -necessarily - needing to utilize further hardware (e.g. more servers or server resources).

It works by storing keys and their matching values (up to 1 MB in size) onto an associative array (i.e. hash table) which can be scaled and distributed across a large number of virtual servers.


Basic Docker Commands
+++

docker CLI Usage:

docker [option] [command] [arguments]


Commands List

Here is a summary of currently available (version 0.7.1) docker commands:


attach        - Attach to a running container
build        - Build a container from a Dockerfile
commit        - Create a new image from a container's changes
cp        - Copy files/folders from the containers filesystem to the host path
diff        - Inspect changes on a container's filesystem
events        - Get real time events from the server
export        - Stream the contents of a container as a tar archive
history        - Show the history of an image
images        - List images
import        - Create a new filesystem image from the contents of a tarball
info        - Display system-wide information
insert        - Insert a file in an image
inspect        - Return low-level information on a container
kill        - Kill a running container
load        - Load an image from a tar archive
login        - Register or Login to the docker registry server
logs        - Fetch the logs of a container
port        - Lookup the public-facing port which is NAT-ed to PRIVATE_PORT
ps        - List containers
pull        - Pull an image or a repository from the docker registry server
push        - Push an image or a repository to the docker registry server
restart        - Restart a running container
rm        - Remove one or more containers
rmi        - Remove one or more images
run        - Run a command in a new container
save        - Save an image to a tar archive
search        - Search for an image in the docker index
start        - Start a stopped container
stop        - Stop a running container
tag        - Tag an image into a repository
top        - Lookup the running processes of a container
version        - Show the docker version information



Getting Started with Creating Memcached Images
+++

Building on our knowledge gained from the previous articles in the docker series, let's dive straight into building a Dockerfile to have docker automatically build Memcached installed images (which will be used to run sandboxed Memcached instances).


Quick Recap: What Are Dockerfiles?
+++

Dockerfiles are scripts containing commands declared successively which are to be executed, in the order given, by docker to automatically create a new docker image. They help greatly with deployments.

These files always begin with the definition of a base image by using the FROM command. From there on, the build process starts and each following action taken forms the final with commits (saving the image state) on the host.

Usage:

        # Build an image using the Dockerfile at current location
        # Tag the final image with [name] (e.g. *nginx*)
        # Example: sudo docker build -t [name] .
        docker build -t memcached_img . 


Dockerfile Commands Overview
+++


ADD        - Copy a file from the host into the container
CMD        - Set default commands to be executed, or passed to the ENTRYPOINT
ENTRYPOINT        - Set the default entrypoint application inside the container
ENV        - Set environment variable (e.g. "key = value")
EXPOSE        - Expose a port to outside
FROM        - Set the base image to use
MAINTAINER        - Set the author / owner data of the Dockerfile
RUN        - Run a command and commit the ending result (container) image
USER        - Set the user to run the containers from the image
VOLUME        - Mount a directory from the host to the container
WORKDIR        - Set the directory for the directives of CMD to be executed


Creating a Dockerfile
+++

Since Dockerfiles constitute of plain-text documents, creating one translates to launching your favourite text editor and writing the commands you want docker to execute in order to build an image. After you start working on the file, continue with adding all the content below (one after the other) before saving the final result.

Note: You can find what the final Dockerfile will look like at the end of this section.

Let's create an empty Dockerfile using nano text editor:

nano Dockerfile

We need to have all instructions (commands) and directives listed successively. However, everything starts with building on a base image (set with the FROM command).

Let's define the purpose of our Dockerfile and declare the base image to use:



        ############################################################
        # Dockerfile to run Memcached Containers
        # Based on Ubuntu Image
        ############################################################

        # Set the base image to use to Ubuntu
        FROM ubuntu

        # Set the file maintainer (your name - the file's author)
        MAINTAINER Maintaner Name



After this initial block of commands and declarations, we can begin with listing the instructions for Memcached installation.


        # Update the default application repository sources list
        RUN apt-get update

        # Install Memcached
        RUN apt-get install -y memcached

Set the default port to be exposed to outside the container:


        # Port to expose (default: 11211)
        EXPOSE 11211

Set the default execution command and entrpoint (i.e. Memcached daemon):


        # Default Memcached run command arguments
        CMD ["-u", "root", "-m", "128"]

        # Set the user to run Memcached daemon
        USER daemon

        # Set the entrypoint to memcached binary
        ENTRYPOINT memcached


Final Dockerfile
+++


        ############################################################
        # Dockerfile to run Memcached Containers
        # Based on Ubuntu Image
        ############################################################

        # Set the base image to use to Ubuntu
        FROM ubuntu

        # Set the file maintainer (your name - the file's author)
        MAINTAINER Maintaner Name

        # Update the default application repository sources list
        RUN apt-get update

        # Install Memcached
        RUN apt-get install -y memcached

        # Port to expose (default: 11211)
        EXPOSE 11211

        # Default Memcached run command arguments
        CMD ["-m", "128"]

        # Set the user to run Memcached daemon
        USER daemon

        # Set the entrypoint to memcached binary
        ENTRYPOINT memcached



Using this Dockerfile, we are ready to get started with dockerised Memcached containers!


Creating the Docker Image for Memcached Containers
+++

We can now create our first Memcached image by following the usage instructions explained in the Dockerfile Basics section.

Run the following command to create an image, tagged as "memcached_img":

$ docker build -t memcached_img .
---

Note: Do not forget the trailing . for docker to find the Dockerfile.


Running dockerised Memcached Containers
+++

It is very simple to create any number of perfectly isolated and self-contained memcached instances - now - thanks to the image we have obtained in the previous section. All we have to do is to create a new container with docker run.


Creating a Memcached Installed Container
+++

To create a new container, use the following command, modifying it to suit your requirements following this example:


# Example: docker run --name [container name] -p [port to access:port exposed] -i -t [memcached image name]

$ docker run --name memcached_ins -d -p 45001:11211 memcached_img
---


Now we will have a docker container named "memcachedins", accessible from port 45001, run using our image tagged "memcachedimg", which we built previously.


Limiting the Memory for a Memcached Container
+++

In order to limit the amount of memory a docker container process can use, simply set the -m [memory amount] flag with the limit.

To run a container with memory limited to 256 MBs:

# Example: sudo docker run -name [name] -m [Memory (int)][memory unit (b, k, m or g)] -d (to run not to attach) -p (to set access and expose ports) [image ID]

$ docker run --name memcached_ins -m 256m -d -p 45001:11211 memcached_img
---


To confirm the memory limit, you can inspect the container:

# Example: docker inspect [container ID] | grep Memory

$ docker inspect memcached_ins | grep Memory
---

Note: The command above will grab the memory related information from the inspection output. To see all the relevant information regarding your container, opt for docker inspect [container ID].


Testing the Memcached Container
+++

There are various ways to try your newly created Memcached running container(s). We will use a simple Python CLI application for this. However, you can just get to production with your application using caching add-ons, frameworks, or libraries.

Make sure that your host has the necessary libraries for Python / Memcached:


		sudo apt-get update && sudo apt-get -y upgrade 
		sudo apt-get install -y python-pip
		pip install python-memcached


Let's create a simple Python script called "mc.py" using nano:

nano cache.py

Copy-and-paste the below (self-explanatory) content inside:


		# Import python-memcache and sys for arguments
		import memcache
		import sys

		# Set address to access the Memcached instance
		addr = 'localhost'

		# Get number of arguments
		# Expected format: python cache.py [memcached port] [key] [value]
		len_argv = len(sys.argv)

		# At least the port number and a key must be supplied
		if len_argv < 3:
			sys.exit("Not enough arguments.")

		# Port is supplied and a key is supplied - let's connect!
		port  = sys.argv[1]
		cache = memcache.Client(["{0}:{1}".format(addr, port)])

		# Get the key
		key   = str(sys.argv[2])

		# If a value is also supplied, set the key-value pair
		if len_argv == 4:

			value = str(sys.argv[3])    
			cache.set(key, value)

			print "Value for {0} set!".format(key)

		# If a value is not supplied, return the value for the key
		else:

			value = cache.get(key)

			print "Value for {0} is {1}.".format(key, value) 



Testing a docker memcached instance using the script above from your host:


        # Example: python cache.py [port] [key] [value]
		python cache.py 45001 my_test_key test_value

        # Return: Value for my_test_key set


        # See if the key is set:
		python cache.py 45001 my_test_key

        # Return: Value for my_test_key is test_value.



Title: docker-explained-how-to-containerize-python-web-applications
===
Source: https://www.digitalocean.com/community/tutorials/docker-explained-how-to-containerize-python-web-applications

Added: Sun Nov  6 18:32:49 CET 2016
Created: PostedDecember 17, 2013

Introduction
+++

The threat of having a web application hijacked and used for taking over the entire host is a vast and scary one. It has long been a challenge to keep things isolated from one another for enhanced security, especially if the applications belong to different clients. Many measures can be taken to prevent this unfortunate scenario, however, they are usually too costly (both on time and resources) or too complicated for most developers' or administrators' use cases.

In this DigitalOcean article, we'll talk about "containerizing" Python web applications in order to have them in very secure sandboxes, absolutely kept within their own environments (unless you explicitly "link" them to another). In order to achieve this, we'll see about creating a docker container to host a Python web application step-by-step, finally bootstrapping our build processes with a Dockerfile to fully automate it.


Building a Docker Container To Sandbox Python WSGI Apps
+++

After having installed docker on our VPS and having quickly gone over its commands, we are ready to start with the actual work to create our docker container running a Python WSGI Application.

Note: The following section will enable you to have a dockerized (containerized) Python WSGI web application. However, it is definitely not the recommended method due to its complexity and impracticability. It is here to offer you a chance to learn how to work with a live container and get familiar with the commands we will need to define later in the next section to automate the process.

Creating a Base Docker Container From Ubuntu
+++

Using docker's RUN command, we will begin with creating a new container based on the Ubuntu image. We are going to attach a terminal to it using the -t flag and will have bash as the running process.

We are going to expose port 80 so that our application will be accessible from the outside. In future, you might want to load-balance multiple instances and "link" containers to each other to access them using a reverse-proxy running container, for example.


$ docker run -i -t -p 80:80 ubuntu /bin/bash
---

Note: After executing this command, docker might need to pull the Ubuntu image before creating a new container for you.

CTRL+P followed by CTRL+Q -detach from running container
---

Remember: You will be attached to the container you create. In order to detach yourself and go back to your main terminal access point, run the escape sequence: CTRL+P followed by CTRL+Q. Being attached to a docker container is like being connected to a new droplet from inside another.

$ docker attach [id] -attach to running container
---

To attach yourself back to this container:

    - List all running containers using "docker ps"
    - Find its ID
    - Use "docker attach [id]" to attach back to its terminal

Important: Please do not forget that since we are in a container, all the following commands will be executed there, without affecting the host it resides.


Preparing the Base Container for the Installation
+++

Update the list with the newly added source.

        apt-get update


Before we proceed with setting up Python WSGI applications, there are some tools we should have such as nano, tar, curl, etc. - just in case.

Let's download some useful tools inside our container.


		apt-get install -y tar \
                           git \
                           curl \
                           nano \
                           wget \
                           dialog \
                           net-tools
                           build-essential



Installing Common Python Tools for Deployment
+++

For our tutorial (as an example), we are going to create a very basic Flask application. After following this article, you can use and deploy your favorite framework instead, the same way you would deploy it on a virtual server.

Remember: All the commands and instructions below still take place inside a container, which acts almost as if it is a brand new droplet of its own.

Let's begin our deployment process with installing Python and pip the Python package manager:


        # Install pip's dependency: setuptools:
        apt-get install -y python python-dev python-distribute python-pip


Installing The Web Application and Its Dependencies
+++

Before we begin with creating a sample application, we better make sure that everything - i.e. all the dependencies - are there. First and foremost, you are likely to have your Web Application Framework (WAF) as your application's dependency (i.e. Flask).

As we have pip installed and ready to work, we can use it to pull all the dependencies and have them set up inside our container:


        # Download and install Flask framework:
        pip install flask



After installing pip, let's create a basic, sample Flask application inside a "my_application" folder which is to contain everything.


        # Make a my_application folder
        mkdir my_application

        # Enter the folder
        cd my_application 


Let's create a single, one page flask "Hello World!" application using nano.


		# Create a sample (app.py) with nano:
		nano app.py

And copy-and-paste the contents below for this small application we have just mentioned:


		from flask import Flask
		app = Flask(__name__)

		@app.route("/")
		def hello():
			return "Hello World!"

		if __name__ == "__main__":
			app.run()


Note: You can create your a list of your actual application's dependencies using pip. To see how, check out our tutorial Common Python Tools: Using virtualenv, Installing with Pip, and Managing Packages.

https://www.digitalocean.com/community/articles/common-python-tools-using-virtualenv-installing-with-pip-and-managing-packages

Our final application folder structure:


		/my_application
			|
			|- requirements.txt  # File containing list of dependencies
			|- /app              # Application module (which should have your app)
			|- app.py            # WSGI file containing the "app" callable
			|- server.py         # Optional: To run the app servers (CherryPy)



Note: Please see the following section regarding the "server.py" - Configuring your Python WSGI Application .

Remember: This application folder will be created inside the container. When you are automatically building images (see the following section on Dockerfiles), you will need to make sure to have this structure on the host, alongside the Dockerfile.


How to get your application repository and its requirements inside a container
+++

In the above example, we created the application directory inside the container. However, you will not be doing that to deploy your application. You are rather likely to pull its source from a repository.

There are several ways to copy your repository inside a container.

Below explained are two of them:

		# Example [1]
		# Download the application using git:
		# Usage: git clone [application repository URL]
		# Example:
		git clone https://github.com/mitsuhiko/flask/tree/master/examples/flaskr

		# Example [2]
		# Download the application tarball:
		# Usage: wget [application repository tarball URL]
		# Example: (make sure to use an actual, working URL)
		wget http://www.github.com/example_usr/application/tarball/v.v.x

		# Expand the tarball and extract its contents:
		# Usage: tar vxzf [tarball filename .tar (.gz)]
		# Example: (make sure to use an actual, working URL)
		tar vxzf application.tar.gz

		# Download and install your application dependencies with pip.
		# Download the requirements.txt (pip freeze output) and use pip to install them all:
		# Usage: curl [URL for requirements.txt] | pip install -r -
		# Example: (make sure to use an actual, working URL)
		curl http://www.github.com/example_usr/application/requirements.txt | pip install -r -


Configuring your Python WSGI Application
+++

To serve this application, you will need a web server. The web server, which powers the WSGI app, needs to be installed in the same container as the application's other resources. In fact, it will be the process that docker runs.

Note: In this example, we will use CherryPy's built-in production ready HTTP web server due to its simplicity. You can use Gunicorn, CherryPy or even uWSGI (and set them up behind Nginx) by following our tutorials on the subject.

Download and install CherryPy with pip:


		pip install cherrypy


Create a "server.py" to serve the web application from "app.py":


		nano server.py


Copy and paste the contents from below for the server to import your application and start serving it:

		# Import your application as:
		# from app import application
		# Example:

		from app import app

		# Import CherryPy
		import cherrypy

		if __name__ == '__main__':

			# Mount the application
			cherrypy.tree.graft(app, "/")

			# Unsubscribe the default server
			cherrypy.server.unsubscribe()

			# Instantiate a new server object
			server = cherrypy._cpserver.Server()

			# Configure the server object
			server.socket_host = "0.0.0.0"
			server.socket_port = 80
			server.thread_pool = 30

			# For SSL Support
			# server.ssl_module            = 'pyopenssl'
			# server.ssl_certificate       = 'ssl/certificate.crt'
			# server.ssl_private_key       = 'ssl/private.key'
			# server.ssl_certificate_chain = 'ssl/bundle.crt'

			# Subscribe this server
			server.subscribe()

			# Start the server engine (Option 1 *and* 2)

			cherrypy.engine.start()
			cherrypy.engine.block()


And that's it! Now you can have a "dockerized" Python web application securely kept in its sandbox, ready to serve thousands and thousands of client requests by simply running:


		python server.py


This will run the server on the foreground. If you would like to stop it, press CTRL+C.

To run the server in the background, run the following:


		python server.py &


    When you run an application in the background, you will need to use a process manager (e.g. htop) to kill (or stop) it.

Note: To learn more about configuring Python WSGI applications for deployment with CherryPy, check out our tutorial: How to deploy Python WSGI apps Using CherryPy Web Server
https://www.digitalocean.com/community/tutorials/how-to-deploy-python-wsgi-applications-using-a-cherrypy-web-server-behind-nginx

To test that everything is running smoothly, which they should given that all the port allocations are already taken care of, you can visit http://[your droplet's IP] with your browser to see the "Hello World!" message.


Creating the Dockerfile to Automatically Build the Image
+++

As we have mentioned in the previous step, it is certainly not the recommended way to create containers this way for a scalable production deployment. The right way to do can be considered as using Dockerfiles to automate the build process in a structured way.

After having gone through the necessary commands for downloading and installing inside a container, we can use the same knowledge to compose a Dockerfile that docker can use to build an image from, which then can be used to run a Python WSGI application container easily.

Before we start working on the Dockerfile, let's quickly go over the basics.


Dockerfile Basics
+++

Dockerfiles are scripts containing commands declared successively, which are to be executed in that order by docker to automatically create a new docker image. They help greatly with deployments.

These files always begin with defining an base image using the FROM command. From there on, the build process starts and each following action taken forms the final image which will be committed on the host.

		Usage:
		# Build an image using the Dockerfile at current location
		# Tag the final image with [name] (e.g. *nginx*)
		# Example: docker build -t [name] .


$ docker build -t nginx_img . 
---


Dockerfile Commands Overview
+++


		ADD
		Copy a file from the host into the container
		CMD
		Set default commands to be executed, or passed to the ENTRYPOINT
		ENTRYPOINT
		Set the default entrypoint application inside the container
		ENV
		Set environment variable (e.g. "key = value")
		EXPOSE
		Expose a port to outside
		FROM
		Set the base image to use
		MAINTAINER
		Set the author / owner data of the Dockerfile
		RUN
		Run a command and commit the ending result (container) image
		USER
		Set the user to run the containers from the image
		VOLUME
		Mount a directory from the host to the container
		WORKDIR
		Set the directory for the directives of CMD to be executed


Creating the Dockerfile
+++

To create a Dockerfile at the current location using the nano text editor, execute the following command:

		nano Dockerfile

Note: Append all the following lines one after the other to form the Dockerfile.


Defining the Fundamentals
---

Let's begin our Dockerfile by defining the basics (fundamentals) such as the FROM image (i.e. Ubuntu) and the MAINTAINER.

Append the following:



		############################################################
		# Dockerfile to build Python WSGI Application Containers
		# Based on Ubuntu
		############################################################

		# Set the base image to Ubuntu
		FROM ubuntu

		# File Author / Maintainer
		MAINTAINER Maintaner Name


Updating the Repository and Installing necessary packages
---

After updating the default application repository sources list, we can begin our deployment process by getting the basic applications we will need.

For deploying Python WSGI applications, you are extremely likely to need some of the tools which we worked with before (e.g. pip). Let's install them now before proceeding with setting up the framework (i.e. your WAF) and the your web application server (WAS) of choice.

		# Update the sources list
		# Install basic applications
		# Install Python and Basic Python Tools
		RUN apt-get update \
			&& apt-get install -y tar git curl nano wget dialog net-tools build-essential \
			&& apt-get install -y python python-dev python-distribute python-pip


Application Deployment
---

Given that we are building docker images to deploy Python web applications, we can very all take advantage of docker's ADD command to copy the application repository, preferably with a REQUIREMENTS file to quickly get running in one single step.

Note: To package everything together in a single file and not to repeat ourselves, an application folder, structured similarly to the one below might be a good way to go.

Example application folder structure:


		/my_application
			|
			|- requirements.txt  # File containing list of dependencies
			|- /app              # Application module
			|- app.py            # WSGI file containing the "app" callable
			|- server.py         # Optional: To run the app servers (CherryPy)



Note: To see about creating this structure, please roll back up and refer to the section Installing The Web Application and Its Dependencies.

Append the following:


		# Copy the application folder inside the container
		ADD /my_application /my_application


Note: If you want to deploy from an online host git repository, you can use the following command to clone:


		RUN git clone [application repository URL]

Please do not forget to replace the URL placeholder with your actual one.


Bootstrapping Everything
---

After adding the instructions for copying the application, let's finish off with final configurations such as pulling the dependencies from the requirements.txt.


		# Get pip to download and install requirements:
		RUN pip install -r /my_application/requirements.txt

		# Expose ports
		EXPOSE 80

		# Set the default directory where CMD will execute
		WORKDIR /my_application

		# Set the default command to execute
		# when creating a new container
		# i.e. using CherryPy to serve the application
		CMD python server.py


Final Dockerfile
---

In the end, this is what the Dockerfile should look like:


		############################################################
		# Dockerfile to build Python WSGI Application Containers
		# Based on Ubuntu
		############################################################

		# Set the base image to Ubuntu
		FROM ubuntu

		# File Author / Maintainer
		MAINTAINER Maintaner Name

		# Update the sources list
		# Install basic applications
		# Install Python and Basic Python Tools
		RUN apt-get update \
			&& apt-get install -y tar git curl nano wget dialog net-tools build-essential \
			&& apt-get install -y python python-dev python-distribute python-pip

		# Copy the application folder inside the container
		# Note: If you want to deploy from an online host git repository, you can use the following command to clone:
		# RUN git clone [application repository URL]
		ADD /my_application /my_application

		# Get pip to download and install requirements
		RUN pip install -r /my_application/requirements.txt

		# Expose ports
		EXPOSE 80

		# Set the default directory where CMD will execute
		WORKDIR /my_application

		# Set the default command to execute
		# When creating a new container
		# i.e. using CherryPy to serve the application
		CMD python server.py


Using the Dockerfile to Automatically Build Containers
+++

As we first went over in the "basics" section, Dockerfiles' usage consists of calling them with docker build command.

Since we are instructing docker to copy an application folder (i.e. /my_application) from the current directory, we need to make sure to have it alongside this Dockerfile before starting the build process.

This docker image will allow us to quickly create containers running Python WSGI applications with a single command.

To start using it, build a new container image with the following:


$ docker build -t my_application_img . 
---

And using that image - which we tagged myapplicationimg - we can run a new container running the application with:


$ docker run -name my_application_instance -p 80:80 -i -t my_application_img
---

Now you can visit the IP address of your droplet, and your application will be running via a docker container.


		Example:

		# Usage: Visit http://[my droplet's ip]
		http://95.85.10.236/

		Sample Response:

		Hello World!




Title: dockerfile_best-practices
===
Source: https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/

Added: Mon Nov  7 17:15:59 CET 2016
Created: N/A



Title: how-to-dockerise-and-deploy-multiple-wordpress-applications-on-ubuntu
===
Source: https://www.digitalocean.com/community/tutorials/how-to-dockerise-and-deploy-multiple-wordpress-applications-on-ubuntu

Added: Mon Nov  7 17:15:59 CET 2016
Created: PostedFebruary 13, 2014


Introduction
+++

WordPress has become one of the most popularly deployed and used web applications the world has ever seen. Thanks to years of constant development, it is now possible to create a nearly endless amount of different websites (or even web-applications) based on WordPress and its available plug-ins / extensions.

In this DigitalOcean article, using the Docker Linux Container Engine, we are going learn how to dockerise (i.e. package and contain) WordPress applications on Ubuntu cloud servers and discover what probably is the most simple and secure way of deploying multiple WordPress sites on a single host.


WordPress In Brief
+++

WordPress was initially created as an easy to install and use self-publication platform (i.e. a Blogging engine). It has become extremely popular over the years, which lead to development of many 3rd party plugins, turning the tool into a full CMS (Content Management System). Based on WordPress, a lot of different types of web-sites and web-applications can be created with simplicity and deployed with ease.

WordPress is an open-source platform that is developed using the PHP programming language, which surely helped it on its way to success. PHP is currently one of the most common web-site and web-application creation languages and the choice of many companies (including Facebook).

WordPress sites rely on MySQL relational-database to keep their data and there are multiple ways to power a WordPress site given the multiple choices available to run PHP and MySQL together.

In this article, we will go with a tried-and-tested method to create WordPress installed Docker images, which will enable you to run yet-another WordPress site on any VPS, with a single command by using Docker.


Docker client commands
+++


		Client Usage:
		docker [option] [command] [arguments]


Note: Docker needs sudo privileges in order to work as it uses sockets owned by root.
Client Commands

You can get a full list of all available commands by simply calling the client:


		docker


Here is a list of all available commands as of version 0.8.0:

		Commands:
		attach    Attach to a running container
		build     Build a container from a Dockerfile
		commit    Create a new image from a container's changes
		cp        Copy files/folders from the containers filesystem to the host path
		diff      Inspect changes on a container's filesystem
		events    Get real time events from the server
		export    Stream the contents of a container as a tar archive
		history   Show the history of an image
		images    List images
		import    Create a new filesystem image from the contents of a tarball
		info      Display system-wide information
		insert    Insert a file in an image
		inspect   Return low-level information on a container
		kill      Kill a running container
		load      Load an image from a tar archive
		login     Register or Login to the docker registry server
		logs      Fetch the logs of a container
		port      Lookup the public-facing port which is NAT-ed to PRIVATE_PORT
		ps        List containers
		pull      Pull an image or a repository from the docker registry server
		push      Push an image or a repository to the docker registry server
		restart   Restart a running container
		rm        Remove one or more containers
		rmi       Remove one or more images
		run       Run a command in a new container
		save      Save an image to a tar archive
		search    Search for an image in the docker index
		start     Start a stopped container
		stop      Stop a running container
		tag       Tag an image into a repository
		top       Lookup the running processes of a container
		version   Show the docker version information
		wait      Block until a container stops, then print its exit code


Working With Dockerfiles
+++

What Are Dockerfiles?
---

Dockerfiles are scripts containing commands declared successively which are to be executed, in the order given, by Docker to automatically create a new image.

These files always begin with the definition of a base image with the FROM instruction. From there on, the build process starts and each following action forms the final image with commits (i.e. saving the image state).

Dockerfiles can be used with the build command:


		# Build an image using the Dockerfile at current location
		# Tag the final image with [name] (e.g. *wordpress_img*)
		# Example: docker build -t [name] .


$ docker build -t wordpress_img . 
~~~


Dockerfile Commands Overview
---

Dockerfiles work by receiving the below instructions:


		ADD: Copy a file from the host into the container
		CMD: Set default commands to be executed, or passed to the ENTRYPOINT
		ENTRYPOINT: Set the default entrypoint application inside the container
		ENV: Set environment variable (e.g. key = value)
		EXPOSE: Expose a port to outside
		FROM: Set the base image to use
		MAINTAINER: Set the author / owner data of the Dockerfile
		RUN: Run a command and commit the ending result (container) image
		USER: Set the user to run the containers from the image
		VOLUME: Mount a directory from the host to the container
		WORKDIR: Set the directory for the directives of CMD to be executed


Creating WordPress Containers
+++

Pulling The Image
---

For our tutorial, we will be using an out-of-the-box WordPress image called tutum/wordpress. This wordpress image is created using Tutum's Wordpress Image: In order to create containers from this image, we need to pull (download) it first.
https://github.com/tutumcloud/wordpress/blob/master/Dockerfile

Let's pull the image:


$ docker pull tutum/wordpress
~~~

This command will download the underlying base images with all modified layers.

Once the image is ready, by issuing a single command we can create dockerised WordPress instances.


Creating A Publicly Accessible WordPress Container
---

Run the following command to create a container that is reachable from the outside on a port you specify (e.g. 80):


		# Usage: docker run -p [Port Number]:80 tutum/wordpress
		# Example:


$ docker run -p 80:80 tutum/wordpress
~~~

The above command will create a WordPress instance that will accept connections from the outside on the default HTTP port 80.


Creating A Locally Accessible WordPress Container
---

Sometimes it might suit you the best to have containers reachable only locally. This can be useful if you decide to set up a load-balancer or another reverse-proxy to distribute connections across many WordPress instances.

Run the following command to create a locally accessible container.


		# Allocate a port dynamically:
		# Usage: docker run -p 127.0.0.1::80 tutum/wordpress
		# Example:

$ docker run -p 127.0.0.1::80 tutum/wordpress
~~~

Once you execute the above command, Docker will create a container, provide you its ID and then dynamically allocate a port. You can figure out which port the container is using with the port command.


		# Usage: docker port [container ID] [private port number]
		# Example:


$ docker port 9af15d73fdf8a997 80
~~~

		# 127.0.0.1:49156


In this case, the output means that the container is accessible only on the localhost on port 49156. You can use the address, provided in full, to redirect connections from a reverse-proxy.

If you would like to specify a port, just place it in-between the IP address and the private port used by the web server inside (e.g. 80):


		# Usage: docker run -p 127.0.0.1:[local port]:80 tutum/wordpress
		# Example:


$ docker run -p 127.0.0.1:8081:80 tutum/wordpress
~~~

This way, you will have a WordPress instance that is locally accessible at port 8081.

Note: In order to run your container in the background, you also need to add the -d flag after the run command:


$ docker run -d ..
~~~

Otherwise, you will be connected to the container where you will see the output from all the applications running.

In order to leave the container, as shown in the introduction article, you need to use the escape sequence CTRL+P immediately followed by CTRL+Q.

Using the docker ps command, you can get the list of running containers to find your newly instantiated one's ID.

Note: Using the -name [name] arguments, you can tag a container with a name which should free you from dealing with complex container IDs:


$ docker run -d --name new_container_1 ..
~~~


Limiting The Memory Usage For Containers
---

In order to limit the amount of memory a docker container process can use, simply set the -m [memory amount] flag with the limit.

To run a container with memory limited to 256 MBs:


		# Example: docker run --name [name] -m [Memory (int)][memory unit (b, k, m or g)] -d (to run not to attach) -p (to set access and expose ports) [image ID]

$ docker run -m 64m -d -p 8082:80 tutum/wordpress
~~~

To confirm the memory limit, you can inspect the container:


		# Example: docker inspect [container ID] | grep Memory

$ docker inspect 9a7562a361122706 | grep Memory
~~~

Note: The command above will grab the memory related information from the inspection output. To see all the relevant information regarding your container, opt for sudo docker inspect [container ID]. Also, please note that your Linux kernel must support swap limit capabilities for actual limitation to work.


Comments
+++

MY NOTE:
There were some interesting questions in the comments.

April 16, 2014

That's a nice tutorial with some valuable hints, but contrary to it's title it doesn't explain how to run **multiple** WP applications.

Answer:
You need to set up nginx or Apache in front of everything as a reverse proxy. For instance, say you have two Wordpress dockers running. Set them to be available over localhost at two different ports:


$ docker run -p 127.0.0.1:8081:80 tutum/wordpress1
---

$ docker run -p 127.0.0.1:8082:80 tutum/wordpress2
---

If you were using nginx, you'd then do something like:

		server {
			listen 80;

			server_name app1.domain.com;

			location / {
				proxy_pass http://localhost:8081;
				proxy_http_version 1.1;
				proxy_set_header Upgrade $http_upgrade;
				proxy_set_header Connection 'upgrade';
				proxy_set_header Host $host;
				proxy_cache_bypass $http_upgrade;
			}
		}

		server {
			listen 80;

			server_name app2.domain.com;

			location / {
				proxy_pass http://localhost:8082;
				proxy_http_version 1.1;
				proxy_set_header Upgrade $http_upgrade;
				proxy_set_header Connection 'upgrade';
				proxy_set_header Host $host;
				proxy_cache_bypass $http_upgrade;
			}
		}


Title: how-to-run-nginx-in-a-docker-container-on-ubuntu-14-04
===
Source: https://www.digitalocean.com/community/tutorials/how-to-run-nginx-in-a-docker-container-on-ubuntu-14-04

Added: Tue Nov  8 17:38:11 CET 2016
Created: PostedOctober 28, 2015


Introduction
+++

This tutorial shows how to deploy Nginx in a Docker container.

By containerizing Nginx, we cut down on our sysadmin overhead. We will no longer need to manage Nginx through a package manager or build it from source. The Docker container allows us to simply replace the whole container when a new version of Nginx is released. We only need to maintain the Nginx configuration file and our content.

Nginx describes itself as:

		nginx [engine x] is an HTTP and reverse proxy server, a mail proxy server, and a generic TCP proxy server, originally written by Igor Sysoev.

In practice many sysadmins use Nginx to serve web content, from flat-file websites to upstream APIs in NodeJS. In this tutorial we will serve a basic web page, so we can focus on configuring Nginx with a Docker container.


Docker containers are a popular form of a relatively old operations practice: containerization. Containerization differs from virtualization in that virtualization abstracts away the hardware, while containerization abstracts away the base operating system, too. In practical terms this means we can take an application (or group of applications) and wrap them in a container (or containers) to make them modular, portable, composable, and lightweight.

This portability means you can install the Docker Engine (also referred to as Docker Core, and even just Docker) on a wide variety of operating systems, and any functional container written by anyone will run on it.


For the purposes of this article we will be installing the Docker Engine on Ubuntu 14.04.

We will be installing the current stable version of Docker for Ubuntu, which is 1.8.1.

This tutorial is aimed at Nginx users who are new to Docker. If you want just the bare commands for setting up your Nginx container, you can do Step 1 and then jump to Step 5.

If you want to build up to your container step by step and learn about port mapping and detached mode, follow the whole tutorial.


Step1 - Prerequisites
---

Optional: Run the hello-world container to make sure everything in working as expected.

$ sudo docker run hello-world
~~~

You should see output similar to that shown below.

		$ docker run hello-world
		Hello from Docker!
		This message shows that your installation appears to be working correctly.

		To generate this message, Docker took the following steps:
		 1. The Docker client contacted the Docker daemon.
		 2. The Docker daemon pulled the "hello-world" image from the Docker Hub.
		 3. The Docker daemon created a new container from that image which runs the
			executable that produces the output you are currently reading.
		 4. The Docker daemon streamed that output to the Docker client, which sent it
			to your terminal.

		To try something more ambitious, you can run an Ubuntu container with:
		 $ docker run -it ubuntu bash

		Share images, automate workflows, and more with a free Docker Hub account:
		 https://hub.docker.com

		For more examples and ideas, visit:
		 https://docs.docker.com/engine/userguide/


(Optional) Step 2  Reviewing Container Basics: Run, List, Remove
---

This section shows how to run a basic container and then remove it. If you already know how to use Docker in general, and want to skip to the Nginx part, go to Step 5.

We've installed the Docker Client as part of our Docker installation, so we have access to the command line tool that allows us to interact with our containers.

If we run the following command;

$ docker ps -a
~~~

You should get output similar to the following;


		Output:
		CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES
		a3b149c3ddea        hello-world         "/hello"            3 minutes ago      Exited (0) 3 minutes ago                       nostalgic_hopper



We can see some basic information about our container.

You'll notice it has a nonsensical name like nostalgic_hopper; these names are generated automatically if you don't specify one when creating the container.

We can also see in that the hello-world example container was run 3 minutes ago and exited 3 minutes ago.

If we run this container again with this command (replacing nostalgic_hopper with your own container name):

$ docker start nostalgic_hopper
~~~

Then run the command to list containers:

$ docker ps -a
~~~


We should now see that the container has run recently;


		Output:
		CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                     PORTS               NAMES
		a3b149c3ddea        hello-world         "/hello"            4 minutes ago      Exited (0) 9 seconds ago                       nostalgic_hopper



By default, Docker containers run their assigned commands and then exit.

Some containers will be set up to run through a list of tasks and finish, while others will run indefinitely.

Now that we've gone through some Docker basics, let's remove the hello-world image, as we won't be needing it again (remember to replace nostalgic_hopper with your container name, or use your container ID).

$ docker rm nostalgic_hopper
~~~

Next we'll start using Nginx.


(Optional) Step 3  Learning How to Expose the Port
---

In this section we'll download the Nginx Docker image and show you how to run the container so it's publicly accessible as a web server.

By default containers are not accessible from the Internet, so we need to map the container's internal port to the Droplet's port. That's what this section will teach you!

First, though, we'll get the Nginx image.


Run the following command to get the Nginx Docker image:

$ sudo docker pull nginx
~~~


This downloads all the necessary components for the container. Docker will cache these, so when we run the container we don't need to download the container image(s) each time.

Docker maintains a site called Dockerhub, a public repository of Docker files (including both official and user-submitted images).
https://hub.docker.com/

The image we downloaded is the official Nginx one, which saves us from having to build our own image.


Let's start our Nginx Docker container with this command:

$ docker run --name docker-nginx -p 80:80 nginx
~~~



	   - run is the command to create a new container
	   -  The --name flag is how we specify the name of the container (if left blank one is assigned for us, like nostalgic_hopper from Step 2)
	   - the -p option specifies the port we are exposing in the format of -p local-machine-port:internal-container-port. In this case we are mapping Port 80 in the container to Port 80 on the server
	   - nginx is the name of the image on dockerhub (we downloaded this before with the pull command, but Docker will do this automatically if the image is missing)



That's all we need to get Nginx up! Paste the IP address of your Droplet into a web browser and you should see Nginx's "Welcome to nginx!" page.

You'll also notice in your shell session that the log for Nginx is being updated when you make requests to your server, because we're running our container interactively.

Let's hit the break shortcut CTRL+C to get back to our shell session.

If you try to load the page now, you'll get a "connection refused" page. This is because we shut down our container. We can verify this with this command:


$ docker ps -a
~~~

You should see something similar to the output shown below.



		Output
		CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                      PORTS               NAMES
		05012ab02ca1        nginx               "nginx -g 'daemon off"   57 seconds ago      Exited


We can see that our Docker container has exited.

Nginx isn't going to be very useful if we need to be attached to the container image for it to work, so in the next step we'll show you how to detach the container to allow it to run independently.

Remove the existing docker-nginx container with this command:

$ docker rm docker-nginx
~~~

In the next step we'll show you how to run it in detached mode.


(Optional) Step 4 Learning How to Run in Detached Mode
---

Create a new, detached Nginx container with this command:

$ docker run --name docker-nginx -p 80:80 -d nginx
~~~

We added the -d flag to run this container in the background.

The output should simply be the new container's ID.

If we run the list command:

$ docker ps
~~~

We're going to see a couple things in the output we haven't seen before.


		Output
		CONTAINER ID        IMAGE               COMMAND                  CREATED              STATUS              PORTS                         NAMES
		b91f3ce26553        nginx               "nginx -g 'daemon off"   About a minute ago   Up About a minute   0.0.0.0:80->80/tcp, 443/tcp   docker-nginx



We can see that instead of Exited (0) X minutes ago we now have Up About a minute, and we can also see the port mapping.

If we go to our server's IP address again in our browser, we will be able to see the "Welcome to nginx!" page again. This time it's running in the background because we specified the -d flag, which tells Docker to run this container in detached mode.

Now we have a running instance of Nginx in a detached container!

It's not useful enough yet, though, because we can't edit the config file, and the container has no access to any of our website files.

Stop the container by running the following command:

$ docker stop docker-nginx
~~~

Now that the container is stopped (you can check with sudo docker ps -a if you want to be sure), we can remove it by running the following command;

$ docker rm docker-nginx
~~~

Now we'll get to the final version of our container, with a quick stop to generate a custom website file.


Step 5 Building a Web Page to Serve on Nginx
---

In this step, we'll create a custom index page for our website. This setup allows us to have persistent website content that's hosted outside of the (transient) container.

Let's create a new directory for our website content within our home directory, and move to it, by running the commands shown below.


		   $ mkdir -p ~/docker-nginx/html
		   $ cd ~/docker-nginx/html



Now let's create an HTML file (we show the commands for Vim, but you can use any text editor you like).


		   $ vim index.html

Enter insert mode by pressing i. Paste in the content shown below (or feel free to add your own HTML markup).


		<html>
		  <head>
			<link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-MfvZlkHCEqatNoGiOXveE8FIwMzZg4W85qfrfIFBfYc= sha512-dTfge/zgoMYpP7QbHy4gWMEGsbsdZeCXz7irItjcC3sPUFtf0kuFbDz/ixG7ArTxmDjLXDmezHubeNikyKGVyQ==" crossorigin="anonymous">
			<title>Docker nginx Tutorial</title>
		  </head>
		  <body>
			<div class="container">
			  <h1>Hello Digital Ocean</h1>
			  <p>This nginx page is brought to you by Docker and Digital Ocean</p>
			</div>
		  </body>
		</html>



If you're familiar with HTML, you'll see that this is a super basic web page. We've included a <link> tag that is pointed to a CDN for Bootstrap (a CSS framework that gives your web page a collection of responsive styles). You can read more about Bootstrap.

We can save this file now by pressing ESC, and then :wq and ENTER:


		write (w) tells Vim to write the changes to the file
		quit (q) tells Vim to exit

We've now got a simple index page to replace the default Nginx landing page.


Step 6 Linking the Container to the Local Filesystem
---

In this section, we'll put it all together. We'll start our Nginx container so it's accessible on the Internet over Port 80, and we'll connect it to our website content on the server.

Background information about volumes; that is, linking to permanent server content from your container:

Docker allows us to link directories from our virtual machine's local file system to our containers.

In our case, since we want to server web pages, we need to give our container the files to render.

We could copy the files into the container as part of a Dockerfile, or copy them into the container after the fact, but both of these methods leave our website in a static state inside the container. By using Docker's data volumes feature, we can create a symbolic link between the Droplet's filesystem and the container's filesystem. This allows us to edit our existing web page files and add new ones into the directory and our container will automatically access them. If you want to read more about Docker and volumes check out the data volumes documentation.
https://docs.docker.com/engine/tutorials/dockervolumes/

The Nginx container is set up by default to look for an index page at /usr/share/nginx/html, so in our new Docker container, we need to give it access to our files at that location.

Making the link:

To do this, we use the -v flag to map a folder from our local machine (~/docker-nginx/html) to a relative path in the container (/usr/share/nginx/html).

We can accomplish this by running the following command:


$ sudo docker run --name docker-nginx -p 80:80 -d -v ~/docker-nginx/html:/usr/share/nginx/html nginx
~~~


We can see that the new addition to the command -v ~/docker-nginx/html:/usr/share/nginx/html is our volume link.


		- the option -v specifies that we're linking a volume
		- the part to the left of the : is the location of our file/directory on our virtual machine (~/docker-nginx/html)
		- the part to the right of the : is the location that we are linking to in our container (/usr/share/nginx/html)



After running that command, if you now point your browser to your DigitalOcean Droplet's IP address, you should see the first heading of Hello Digital Ocean (or whatever web page you created in Step 5).

If you're happy with the other Nginx defaults, you're all set.

You can upload more content to the ~/docker-nginx/html/ directory, and it will be added to your live website.

For example, if we modify our index file, and if we reload our browser window, we will be able to see it update in realtime. We could build a whole site out of flat HTML files this way if we wanted to. For example, if we added an about.html page, we could access it at http://your_server_ip/about.html without needing to interact with the container.



(Optional) Step 7 Using Your Own Nginx Configuration File
---

This section is for advanced users who want to use their own Nginx config file with their Nginx container. Please skip this step if you don't have a custom config file you want to use.

Let's go back a directory so we aren't writing to our public HTML directory:


		#
		$ cd ~/docker-nginx

If you'd like to take a look at the default config file, just copy it using the Docker copy command:


$ docker cp docker-nginx:/etc/nginx/conf.d/default.conf default.conf
~~~

Since we're going to be using a custom.conf file for Nginx, we will need to rebuild the container.

First stop the container:

$ docker stop docker-nginx
~~~

Remove it with:

$ docker rm docker-nginx
~~~


Now you can edit the default file locally (to serve a new directory, or to use a proxy_pass to forward the traffic to another app/container like you would with a regular Nginx installation). You can read about Nginx's configuration file in our Nginx config file guide.
https://www.digitalocean.com/community/tutorials/understanding-the-nginx-configuration-file-structure-and-configuration-contexts

Once you've saved your custom config file, it's time to make the Nginx container. Simply add a second -v flag with the appropriate paths to give a fresh Nginx container the appropriate links to run from your own config file.


$ docker run --name docker-nginx -p 80:80 -v ~/docker-nginx/html:/usr/share/nginx/html -v ~/docker-nginx/default.conf:/etc/nginx/conf.d/default.conf -d nginx
~~~


This command also still links in the custom website pages to the container.

Please note that you will need to restart the container using a docker restart command if you make any changes to your config file after starting the container, since Nginx does not hot reload if its config file is changed:

$ docker restart docker-nginx
~~~

You now have a running Nginx container serving a custom web page.

From here, we recommend reading up on Docker's container linking if you want to learn about linking containers together for the purposes of using Nginx as a reverse proxy for serving other container-based web apps.
https://docs.docker.com/userguide/dockerlinks/

If you wanted to manage a group of containers, such as an app container, a database container, and this Nginx container, take a look at Docker Compose.
https://docs.docker.com/compose/



Title: how-to-install-and-use-docker-compose-on-ubuntu-14-04
===
Source: https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-compose-on-ubuntu-14-04

Added: Thu Nov 10 12:16:34 CET 2016
Created: PostedNovember 19, 2015


Introduction
+++

Docker is a great tool, but to really take full advantage of its potential it's best if each component of your application runs in its own container. For complex applications with a lot of components, orchestrating all the containers to start up and shut down together (not to mention talk to each other) can quickly become unwieldy. 

The Docker community came up with a popular solution called Fig, which allowed you to use a single YAML file to orchestrate all your Docker containers and configurations. This became so popular that the Docker team eventually decided to make their own version based on the Fig source. They called it Docker Compose. In short, it makes dealing with the orchestration processes of Docker containers (such as starting up, shutting down, and setting up intra-container linking and volumes) really easy.

By the end of this article, you will have Docker and Docker Compose installed and have a basic understanding of how Docker Compose works.


Docker and Docker Compose Concepts
+++

Using Docker Compose requires a combination of a bunch of different Docker concepts in one, so before we get started let's take a minute to review the various concepts involved. If you're already familiar with Docker concepts like volumes, links, and port forwarding then you might want to go ahead and skip on to the next section.

Docker Images
---

Each Docker container is a local instance of a Docker image. You can think of a Docker image as a complete Linux installation. Usually a minimal installation contains only the bare minimum of packages needed to run the image. These images use the kernel of the host system, but since they are running inside a Docker container and only see their own file system, it's perfectly possible to run a distribution like CentOS on an Ubuntu host (or vice-versa).

Most Docker images are distributed via the Docker Hub, which is maintained by the Docker team. Most popular open source projects have a corresponding image uploaded to the Docker Registry, which you can use to deploy the software. When possible it's best to grab "official" images, since they are guaranteed by the Docker team to follow Docker best practices.


Communication Between Docker Images
---

Docker containers are isolated from the host machine by default, meaning that by default the host machine has no access to the file system inside the Docker container, nor any means of communicating with it via the network. Needless to say, this makes configuring and working with the image running inside a Docker container difficult by default.

Docker has three primary ways to work around this. The first and most common is to have Docker specify environment variables that will be set inside the Docker container. The code running inside the Docker container will then check the values of these environment variables on startup and use them to configure itself properly. 


Another commonly used method is a Docker data volume.
https://www.digitalocean.com/community/tutorials/how-to-work-with-docker-data-volumes-on-ubuntu-14-04

Docker volumes come in two flavors — internal and shared. Docker containers run ephemerally by default, which means that every time the container is shut down or restarted it doesn't save its data — it essentially reverts to the state it was in when the container started. This is good for testing and development since you're guaranteed to be working with the same environment on each run of your Docker container but not so good if you were hoping the blog posts you happily typed into your WordPress install would still be there next time you restart Docker. (Luckily the creators of the official Docker WordPress and MariaDB images have taken care of this issue for us. More on that in the Deploying Wordpress and PHPMyAdmin with Docker Compose on Ubuntu 14.04 article.)
https://www.digitalocean.com/community/tutorials/how-to-install-wordpress-and-phpmyadmin-with-docker-compose-on-ubuntu-14-04

Specifying an internal volume just means that for a folder you specify for a particular Docker container, the data will be persisted between restarts. For example if you wanted to make sure your log files hung around between each restart you might specify an internal /var/log volume.

A shared volume maps a folder inside a Docker container onto a folder on the host machine. This allows you to easily share files between the Docker container and the host machine, which we'll explore in the Docker data volume article.
https://www.digitalocean.com/community/tutorials/how-to-work-with-docker-data-volumes-on-ubuntu-14-04

The third way to communicate with a Docker container is via the network. Docker allows communication between different Docker containers via links, as well as port forwarding, allowing you to forward ports from inside the Docker container to ports on the host server. For example, you can create a link to allow your WordPress and MariaDB Docker containers to talk to each other and port-forwarding to expose WordPress to the outside world so that users can connect to it. 


Installing Docker Compose
+++

Now that you have Docker installed, let's go ahead and install Docker Compose. 

From Github
---

MY NOTE:
https://docs.docker.com/compose/install/

Compose can be installed from pypi using pip. If you install using pip it is highly recommended that you use a virtualenv because many operating systems have python system packages that conflict with docker-compose dependencies. See the virtualenv tutorial to get started.

Based on that seems that the variant of instalation from Github (described below) is the preferred one.

$ curl -L "https://github.com/docker/compose/releases/download/1.8.1/docker-compose-$(uname -s)-$(uname -m)" > /usr/local/bin/docker-compose
~~~

$ chmod +x /usr/local/bin/docker-compose
~~~

Test the installation.

$ docker-compose --version
~~~

docker-compose version: 1.8.1


Alternate method via python-pip
---

First, install python-pip as prerequisite:


$ sudo apt-get -y install python-pip
~~~

Then you can install Docker Compose:

$ sudo pip install docker-compose
~~~


Running a Container with Docker Compose
+++

The public Docker registry, Docker Hub, includes a simple Hello World image. Now that we have Docker Compose installed, let's test it with this really simple example.

First, create a directory for our YAML file:


        #
        $ mkdir hello-world

Then change into the directory:


        #
        $ cd hello-world

Now create the YAML file using your favorite text editor (we will use nano):


        #
        $ nano docker-compose.yml

Put the following contents into the file, save the file, and exit the text editor:


        my-test:
          image: hello-world


The output should start with the following:

        
        $ docker-compose up
        Creating helloworld_my-test_1
        Attaching to helloworld_my-test_1
        my-test_1  | 
        my-test_1  | Hello from Docker!
        my-test_1  | This message shows that your installation appears to be working correctly.
        my-test_1  | 
        my-test_1  | To generate this message, Docker took the following steps:
        my-test_1  |  1. The Docker client contacted the Docker daemon.
        my-test_1  |  2. The Docker daemon pulled the "hello-world" image from the Docker Hub.
        my-test_1  |  3. The Docker daemon created a new container from that image which runs the
        my-test_1  |     executable that produces the output you are currently reading.
        my-test_1  |  4. The Docker daemon streamed that output to the Docker client, which sent it
        my-test_1  |     to your terminal.
        my-test_1  | 
        my-test_1  | To try something more ambitious, you can run an Ubuntu container with:
        my-test_1  |  $ docker run -it ubuntu bash
        my-test_1  | 
        my-test_1  | Share images, automate workflows, and more with a free Docker Hub account:
        my-test_1  |  https://hub.docker.com
        my-test_1  | 
        my-test_1  | For more examples and ideas, visit:
        my-test_1  |  https://docs.docker.com/engine/userguide/
        my-test_1  | 
        helloworld_my-test_1 exited with code 0


This simple test does not show one of the main benefits of Docker Compose — being able to bring a group of Docker containers up and down all at the same time. The How To Install Wordpress and PhpMyAdmin with Docker Compose on Ubuntu 14.04 articles show how to use Docker Compose to run three containers as one application group.
https://www.digitalocean.com/community/tutorials/how-to-install-wordpress-and-phpmyadmin-with-docker-compose-on-ubuntu-14-04


Learning Docker Compose Commands
+++

Let's go over the commands the docker-compose tool supports.

The docker-compose command works on a per-directory basis. You can have multiple groups of Docker containers running on one machine — just make one directory for each container and one docker-compose.yml file for each container inside its directory.

So far we've been running docker-compose up on our own and using CTRL-C to shut it down. This allows debug messages to be displayed in the terminal window. This isn't ideal though, when running in production you'll want to have docker-compose act more like a service. One simple way to do this is to just add the -d option when you up your session:


$ docker-compose up -d
---

docker-compose will now fork to the background.

To show your group of Docker containers (both stopped and currently running), use the following command:

$ docker-compose ps
---


For example, the following shows that the helloworld_my-test_1 container is stopped:


		Output of `docker-compose ps`
				Name           Command   State    Ports 
		-----------------------------------------------
		helloworld_my-test_1   /hello    Exit 0         


A running container will show the Up state:

		Output of `docker-compose ps`
			 Name              Command          State        Ports      
		---------------------------------------------------------------
		nginx_nginx_1   nginx -g daemon off;   Up      443/tcp, 80/tcp 


To stop all running Docker containers for an application group, issue the following command in the same directory as the docker-compose.yml file used to start the Docker group:

$ docker-compose stop
---


Note: docker-compose kill is also available if you need to shut things down more forcefully.

In some cases, Docker containers will store their old information in an internal volume. If you want to start from scratch you can use the rm command to fully delete all the containers that make up your container group:


$ docker-compose rm 
---

If you try any of these commands from a directory other than the directory that contains a Docker container and .yml file, it will complain and not show you your containers:


		Output from wrong directory
		Can't find a suitable configuration file in this directory or any parent. Are you in the right directory?
		Supported filenames: docker-compose.yml, docker-compose.yaml, fig.yml, fig.yaml


Accessing the Docker Container Filesystem (Optional)
+++

If you need to work on the command prompt inside a container, you can use the docker exec command.

The Hello World! example exits after it is run, so we need to start a container that will keep running so we can then use docker exec to access the filesystem for the container. Let's take a look at the Nginx image from Docker Hub.

Create a new directory for it and change into it:


		#
		$ mkdir ~/nginx && cd $_

Create a docker-compose.yml file in our new directory:


		#
		$ nano docker-compose.yml

and paste in the following:


		nginx:
		  image: nginx


Save the file and exit. We just need to start the Nginx container as a background process with the following command:

$ docker-compose up -d
---

The Nginx image will be downloaded and then the container will be started in the background.

Now we need the CONTAINER ID for the container. List of all the containers that are running:

$ docker ps
---

You will see something similar to the following:

		Output of `docker ps`
		CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES
		e90e12f70418        nginx               "nginx -g 'daemon off"   6 minutes ago       Up 5 minutes        80/tcp, 443/tcp     nginx_nginx_1


Note: Only running containers are listed with the docker ps command.


If we wanted to make a change to the filesystem inside this container, we'd take its ID (in this example e90e12f70418) and use docker exec to start a shell inside the container:


$ docker exec -it e90e12f70418 /bin/bash
---

The -t option opens up a terminal, and the -i option makes it interactive. The /bin/bash options opens a bash shell to the running container. Be sure to use the ID for your container.

You will see a bash prompt for the container similar to:


		#
		root@e90e12f70418:/#

From here, you can work from the command prompt. Keep in mind, however, that unless you are in a directory that is saved as part of a data volume, your changes will disappear as soon as the container is restarted. Another caveat is that most Docker images are created with very minimal Linux installs, so some of the command line utilities and tools you are used to may not be present. 


Conclusion

Great, so that covers the basic concepts of Docker Compose and how to get it installed and running. Check out the Deploying Wordpress and PHPMyAdmin with Docker Compose on Ubuntu 14.04 tutorial for a more complicated example of how to deploy an application with Docker Compose.
https://www.digitalocean.com/community/tutorials/how-to-install-wordpress-and-phpmyadmin-with-docker-compose-on-ubuntu-14-04


For a complete list of configuration options for the docker-compose.yml file refer to the Compose file reference.
https://docs.docker.com/compose/compose-file/




Title: how-to-work-with-docker-data-volumes-on-ubuntu-14-04
===
Source: https://www.digitalocean.com/community/tutorials/how-to-work-with-docker-data-volumes-on-ubuntu-14-04

Added: Thu Nov 10 13:45:45 CET 2016
Created: PostedNovember 19, 2015



Introduction
+++

In this article we're going to run through the concept of Docker data volumes: what they are, why they're useful, the different types of volumes, how to use them, and when to use each one. We'll also go through some examples of how to use Docker volumes via the docker command line tool.

By the time we reach the end of the article, you should be comfortable creating and using any kind of Docker data volume.


Explaining Docker Containers
+++

Working with Docker requires understanding quite a few Docker-specific concepts, and most of the documentation focuses on explaining how to use Docker's toolset without much explanation of why you'd want to use any of those tools. This can be confusing if you're new to Docker, so we'll start by going through some basics and then jump into working with Docker containers. Feel free to skip ahead to the next section if you've worked with Docker before and just want to know how to get started with data volumes.

A Docker container is similar to a virtual machine. It basically allows you to run a pre-packaged "Linux box" inside a container. The main difference between a Docker container and a typical virtual machine is that Docker is not quite as isolated from the surrounding environment as a normal virtual machine would be. A Docker container shares the Linux kernel with the host operating system, which means it doesn't need to "boot" the way a virtual machine would.

Since so much is shared, firing up a Docker container is a quick and cheap operation — in most cases you can bring up a full Docker container (the equivalent of a normal virtual machine) in the same time as it would take to run a normal command line program. This is great because it makes deploying complex systems a much easier and more modular process, but it's a different paradigm from the usual virtual machine approach and has some unexpected side effects for people coming from the virtualization world. 


Understanding Docker Data Volumes
+++

The biggest point of confusion is that Docker filesystems are temporary by default. If you start up a Docker image you'll end up with a container that on the surface behaves much like a virtual machine. You can create, modify, and delete files to your heart's content. But if you stop the container and start it up again (MY NOTE: I think he was referring here to creating a new container from the image.), all your changes will be lost: any files you previously deleted will now be back, and any new files or edits you made won't be present. This is because Docker images are more like templates than like images in the standard virtualization world. 

Docker images are the files that make up an application. For example, if you download the Docker Nginx image from the Docker Hub (the public Docker Registry), you'll be downloading an image that contains a minimal Linux distribution and the binaries that Nginx itself needs to run. A container is like another layer that sits on top of this image without ever modifying the original image itself. If you're familiar with the concept of snapshots in VirtualBox or other virtualization tools, this works similarly. To save your changes you actually have to create a new Docker image (not container) that includes your newly changed files.

That's fine if you're interested in creating new Docker images, but what if you just want to do something like use the public Nginx Docker image to serve your own webpage? Having to copy your webpages into place every single time you start your Docker container is far from ideal. This is where data volumes come in. Docker data volumes provide you with a separate place to store other data. You can tell Docker to use any folder as a container, and from then on any files written to that location will be persisted. 


Working Without Docker Data Volumes
+++

We mentioned earlier that files in Docker containers are temporary. This is a weird concept, so let's take a second to see the problem in action. If you've already seen files unexpectedly disappear from your Docker containers and just want to know how to fix it then go ahead and skip on to the next section.

Let's run a basic ubuntu image and watch how files we create disappear when the container is killed.

$ docker run -t -i ubuntu /bin/bash
---


This command does a bunch of things in one go. The run portion with the -t -i options tells Docker to prepare to run an image in interactive mode with a terminal. The ubuntu command line option is the name of the image to run, and the /bin/bash is the name of the executable inside that image that we want to start. Docker will automatically attempt to download images that it can't find locally from the public Docker Registry, so running this command will result in Docker downloading an Ubuntu image from the Docker Registry and then launching bash inside it, which gives us a standard shell.

The prompt will change to something similar to:


		Ubuntu container bash shell
		root@cbceb444ec6a:/#


Let's go ahead and write a file to the root of the filesystem inside the Docker container:


		#
		$ echo "I'm going to disappear" > /byebye

You can see that the file /byebye was created and its contents are present via the usual shell commands. For example, inside the bash shell for your Ubuntu container, if you list the contents of the / directory with the command:


		#
    	$ ls /

You will see the following:


		Output of ls /
		bin  boot  byebye  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var


If you issue the command:

		#
		$ cat /byebye

You will see the following:


		Output of `cat /byebye`
		I'm going to disappear

Go ahead and type exit at the shell now, and you'll be dropped back to your host system's shell.

Now, scroll back up and repeat the steps in this section again, except don't create the file /byebye again. You will see that it no longer exists. For example, the command:


		#
		$ cat /byebye

now shows the following:


		Output of cat /byebye
		cat: /byebye: No such file or directory

MY NOTE: The file does not exist, because you are in a different container, if you start the initially created container, the file would still be there, afaik.

Be sure to exit from the Ubuntu container bash shell:


		$ exit


Learning the Types of Docker Data Volumes
+++

There are three main use cases for Docker data volumes:

   - To keep data around, even through container restarts
   - To share data between the host filesystem and the Docker container
   - To share data with other Docker containers

The third case is a little more advanced, so we won't go into it in this tutorial, but the first two are quite common.

In the first (and simplest) case you just want the data to hang around even if you shut down or restart the container, so it's often easiest to let Docker manage where the data gets stored. 


Keeping Data Persistent
+++

There's no way to directly create a "data volume" in Docker, so instead we create a data volume container with a volume attached to it. For any other containers that you then want to connect to this data volume container, use the Docker's --volumes-from option to grab the volume from this container and apply them to the current container. This is a bit unusual at first glance, so let's run through a quick example of how we could use this approach to make our byebye file stick around even if the container is restarted.

First, create a new data volume container to store our volume:

$ docker create -v /tmp --name datacontainer ubuntu
---


This created a container named datacontainer based off of the ubuntu image and in the directory /tmp.

Now, if we run a new Ubuntu container with the --volumes-from flag and run bash again as we did earlier, anything we write to the /tmp directory will get saved to the /tmp volume of our datacontainer container.

First, start the ubuntu image:

$ docker run -t -i --volumes-from datacontainer ubuntu /bin/bash
---

The -t command line options calls a terminal from inside the container. The -i flag makes the connection interactive.

At the bash prompt for the ubuntu container, create a file in /tmp:


		#
		$ echo "I'm not going anywhere" > /tmp/hi


Go ahead and type exit to return to your host machine's shell. Now, run the same command again:


$ docker run -t -i --volumes-from datacontainer ubuntu /bin/bash
---

This time the hi file is already there:


		#
		$ cat /tmp/hi

You should see:


		Output of cat /tmp/hi
		I'm not going anywhere


You can add as many --volumes-from flags as you'd like (for example, if you wanted to assemble a container that uses data from multiple data containers). You can also create as many data volume containers as you'd like.

The only caveat to this approach is that you can only choose the mount path inside the container (/tmp in our example) when you create the data volume container. 


MY NOTE: 
The author of this article is providing misleading info, as in his scenario, he always creates a new container, so it's expected that the file created in a different previously created container, doesn't exist in a new container, since a new container is just a fresh instance of the base image, and doesn't have anything else in it, it's like a new born. Basically what his example is showing, is that he created a separate container "datacontainer", and is writing to this container, so data in that container is persisting, which is totally opposite to what the author is saying, since he is saying that data in a container doesn't persist, which is not correct since the data in a container does persist. This was also addressed in the comments section.


Comments
---

December 14, 2015


		Hi~ Thank you for your sharing!
		I'm new to Docker, but I think that there is something different from what I know about how the modifications to the container will be dealt with. I noticed that you use "docker run" during the whole process with out an argument "--name <name>", thus, Docker will create a new container every time, whose name is a random one generated by the docker itself, eg: if you run

		docker run -d busybox && docker ps -a

		there comes a container whose name is "sleepyliskov" on my machine, you run it again, another container called "ferventlalande" is created, they exit at once cause the busybox dose nothing. So, my point is, in your scenario, when you created the file "byebye" in a container without a name given by you, you stop it, and you "docker run" again, the new container is a new one which is definitely not the one with the "byebye" file. But if you use

		docker ps -a

		you'll find the container(let's call it foo_bar) with the "byebye" file, it's not gone, just stopped. you can use

		docker start foo_bar

		to restart it.
		So, when you say stop the container and start it up again, I would think that you mean "docker start " and "docker stop" not "docker run", the container will be always there with any of your modification until you remove them with

		docker rm <name>

		So, I guess maybe you try to find the "byebye " file in the wrong place, and it may be a good practice to use the "--name" argument or use the "--rm". 



April 15, 2016

		
		#
		You are correct, author of the article created a completely new container instead of looking for the file in the existing cotainer


June 17, 2016

		

		Is this still up to date? I think the changes are now persisted in the top RW layer, even if no data volume is mounted. Right?
		I tried creating a file an restarting the container. The file was still there...





Sharing Data Between the Host and the Docker Container
+++

The other common use for Docker containers is as a means of sharing files between the host machine and the Docker container. This works differently from the last example. There's no need to create a "data-only" container first. You can simply run a container of any Docker image and override one of its directories with the contents of a directory on the host system.


As a quick real-world example, let's say you wanted to use the official Docker Nginx image but you wanted to keep a permanent copy of the Nginx's log files to analyze later. By default the nginx Docker image logs to the /var/log/nginx directory, but this is /var/log/nginx inside the Docker Nginx container. Normally it's not reachable from the host filesystem. 


Let's create a folder to store our logs and then run a copy of the Nginx image with a shared volume so that Nginx writes its logs to our host's filesystem instead of to the /var/log/nginx inside the container:


		#
		$ mkdir ~/nginxlogs


Then start the container: 


$ docker run -d -v ~/nginxlogs:/var/log/nginx -p 5000:80 -i nginx
---


This run command is a little different from the ones we've used so far, so let's break it down piece by piece:




	- -v ~/nginxlogs:/var/log/nginx  We set up a volume that links the /var/log/nginx directory from inside the Nginx container to the ~/nginxlogs directory on the host machine. Docker uses a : to split the host's path from the container path, and the host path always comes first.

	- -d  Detach the process and run in the background. Otherwise, we would just be watching an empty Nginx prompt and wouldn't be able to use this terminal until we killed Nginx.

	- -p 5000:80  Setup a port forward. The Nginx container is listening on port 80 by default, and this maps the Nginx container's port 80 to port 5000 on the host system.


If you were paying close attention, you may have also noticed one other difference from the previous run commands. Up until now we've been specifying a command at the end of all our run statements (usually /bin/bash) to tell Docker what command to run inside the container. Because the Nginx image is an official Docker image, it follows Docker best practices, and the creator of the image set the image to run the command to start Nginx automagically. We can just drop the usual /bin/bash here and let the creators of the image choose what command to run in the container for us.

So, we now have a copy of Nginx running inside a Docker container on our machine, and our host machine's port 5000 maps directly to that copy of Nginx's port 80. Let's use curl to do a quick test request:


		#
		$ curl localhost:5000

You'll get a screenful of HTML back from Nginx showing that Nginx is up and running. But more interestingly, if you look in the ~/nginxlogs folder on the host machine and take a look at the access.log file you'll see a log message from Nginx showing our request:


		#
		$ cat ~/nginxlogs/access.log


You will see something similar to:


		Output of `cat ~/nginxlogs/access.log`
		172.17.42.1 - - [23/Oct/2015:05:22:51 +0000] "GET / HTTP/1.1" 200 612 "-" "curl/7.35.0" "-"


If you make any changes to the ~/nginxlogs folder, you'll be able to see them from inside the Docker container in real-time as well. 


Conclusion

That about sums it up! We've now covered how to create data volume containers whose volumes we can use as a way to persist data in other containers as well as how to share folders between the host filesystem and a Docker container. This covers all but the most advanced use cases when it comes to Docker data volumes.

If you are using Docker Compose, Docker data volumes can be configured in your docker-compose.yml file. Check out How To Install and Use Docker Compose on Ubuntu 14.04 for details.




Title: how-to-install-wordpress-and-phpmyadmin-with-docker-compose-on-ubuntu-14-04
===
Source: https://www.digitalocean.com/community/tutorials/how-to-install-wordpress-and-phpmyadmin-with-docker-compose-on-ubuntu-14-04

Added: 
Thu Nov 10 16:20:04 CET 2016
Created: PostedNovember 19, 2015



Introduction
+++

Docker Compose makes dealing with the orchestration processes of Docker containers (such as starting up, shutting down, and setting up intra-container linking and volumes) really easy.

This article provides a real-world example of using Docker Compose to install an application, in this case WordPress with PHPMyAdmin as an extra. WordPress normally runs on a LAMP stack, which means Linux, Apache, MySQL/MariaDB, and PHP. The official WordPress Docker image includes Apache and PHP for us, so the only part we have to worry about is MariaDB.


Installing WordPress
+++

We'll be using the official WordPress and MariaDB Docker images. If you're curious, there's lots more info about these images and their configuration options on their respective GitHub and Docker Hub pages.
https://hub.docker.com/_/wordpress/
https://hub.docker.com/_/mariadb/

Let's start by making a folder where our data will live and creating a minimal docker-compose.yml file to run our WordPress container:


		#
		mkdir ~/wordpress && cd $_

Then create a ~/wordpress/docker-compose.yml with your favorite text editor (nano is easy if you don't have a preference):


		#
		nano ~/wordpress/docker-compose.yml

and paste in the following:
 

		wordpress:
		  image: wordpress


This just tells Docker Compose to start a new container called wordpress and download the wordpress image from the Docker Hub.

We can bring the image up like so:


$ docker-compose up
---


You'll see Docker download and extract the WordPress image from the Docker Hub, and after some time you'll get some error messages similar to the below:


		$ docker-compose up
		Pulling wordpress (wordpress:latest)...
		latest: Pulling from library/wordpress
		386a066cd84a: Pull complete
		269e95c6053a: Pull complete
		6243d5c57a34: Pull complete
		872f6d38a33b: Pull complete
		e5ea5361568c: Pull complete
		f81f18e77719: Pull complete
		f9dbc878ca0c: Pull complete
		195935e4100b: Pull complete
		c047d6392f67: Pull complete
		6d5afcbf41ee: Pull complete
		bbe672c318f3: Pull complete
		c015a3b2e201: Pull complete
		6eb6d78a72af: Pull complete
		ff76b754471b: Pull complete
		a95632e24e80: Pull complete
		863f9b4c6b73: Pull complete
		8b02b7760190: Pull complete
		58725be21a3b: Pull complete
		8e7c95e1471c: Pull complete
		Digest: sha256:f5e8cafe19d2ff082445960169ceaf7eac01a0669927f963896d6572c15bd607
		Status: Downloaded newer image for wordpress:latest
		Creating wordpress_wordpress_1
		Attaching to wordpress_wordpress_1
		wordpress_1  | error: missing required WORDPRESS_DB_PASSWORD environment variable
		wordpress_1  |   Did you forget to -e WORDPRESS_DB_PASSWORD=... ?
		wordpress_1  | 
		wordpress_1  |   (Also of interest might be WORDPRESS_DB_USER and WORDPRESS_DB_NAME.)
		wordpress_wordpress_1 exited with code 1

This is WordPress complaining that it can't find a database. Let's add a MariaDB image to the mix and link it up to fix that.


Installing MariaDB
+++

To add the MariaDB image to the group, re-open docker-compose.yml with your text editor:


		#
		$ nano ~/wordpress/docker-compose.yml

Change docker-compose.yml to match the below (be careful with the indentation, YAML files are white-space sensitive)
docker-compose.yml


		wordpress:
		  image: wordpress
		  links:
			- wordpress_db:mysql
		wordpress_db:
		  image: mariadb



What we've done here is define a new container called wordpress_db and told it to use the mariadb image from the Docker Hub. We also told the our wordpress container to link our wordpress_db container into the wordpress container and call it mysql (inside the wordpress container the hostname mysql will be forwarded to our wordpress_db container).

If you run docker-compose up again, you will see it download the MariaDB image, and you'll also see that we're not quite there yet though:


		Output
		wordpress_db_1 | error: database is uninitialized and MYSQL_ROOT_PASSWORD not set
		wordpress_db_1 |   Did you forget to add -e MYSQL_ROOT_PASSWORD=... ?
		wordpress_1    | error: missing required WORDPRESS_DB_PASSWORD environment variable
		wordpress_1    |   Did you forget to -e WORDPRESS_DB_PASSWORD=... ?
		wordpress_1    | 
		wordpress_1    |   (Also of interest might be WORDPRESS_DB_USER and WORDPRESS_DB_NAME.)
		wordpress_wordpress_db_1 exited with code 1
		wordpress_wordpress_1 exited with code 1
		Gracefully stopping... (press Ctrl+C again to force)


WordPress is still complaining about being unable to find a database, and now we have a new complaint from MariaDB saying that no root password is set.

It appears that just linking the two containers isn't quite enough. Let's go ahead and set the MYSQL_ROOT_PASSWORD variable so that we can actually fire this thing up.

Edit the Docker Compose file yet again:


		#
		$ nano ~/wordpress/docker-compose.yml

Add these two lines to the end of the wordpress_db section, but make sure to change examplepass to a more secure password!


		#
		wordpress_db:
		...
		  environment:
			MYSQL_ROOT_PASSWORD: examplepass
		...


This will set an environment variable inside the wordpress_db container called MYSQL_ROOT_PASSWORD with your desired password. The MariaDB Docker image is configured to check for this environment variable when it starts up and will take care of setting up the DB with a root account with the password defined as MYSQL_ROOT_PASSWORD.

While we're at it, let's also set up a port forward so that we can connect to our WordPress install once it actually loads up. Under the wordpress section add these two lines:
docker-compose.yml


		#
		wordpress:
		...
		  ports:
			- 8080:80
		...



The first port number is the port number on the host, and the second port number is the port inside the container. So, this configuration forwards requests on port 8080 of the host to the default web server port 80 inside the container.

Note: If you would like Wordpress to run on the default web server port 80 on the host, change the previous line to 80:80 so that requests to port 80 on the host are forwarded to port 80 inside the Wordpress container.


Your complete docker-compose.yml file should now look like this:


		#
		wordpress:
		  image: wordpress
		  links:
			- wordpress_db:mysql
		  ports:
			- 8080:80
		wordpress_db:
		  image: mariadb
		  environment:
			MYSQL_ROOT_PASSWORD: examplepass


With this configuration we can actually go ahead and fire up WordPress. This time, let's run it with the -d option, which will tell docker-compose to run the containers in the background so that you can keep using your terminal:


$ docker-compose up -d
---

You'll see a whole bunch of text fly by your screen. Once it's calmed down, open up a web browser and browse to the IP
of your DigitalOcean box on port 8080 (for example, if the IP address of your server is 123.456.789.123 you should type http://123.456.789.123:8080 into your browser.)

You should see a fresh WordPress installation page and be able to complete the install and blog as usual.

Because these are both official Docker images and are following all of Docker's best practices, each of these images have pre-defined, persistent volumes for you — meaning that if you restart the container, your blog posts will still be there. You can learn more about working with Docker volumes in the Docker data volumes tutorial.


Adding a PhpMyAdmin Container
+++

Great, that was relatively painless. Let's try getting a little fancy.

So far we've only been using official images, which the Docker team takes great pains to ensure are accurate. You may have noticed that we didn't have to give the WordPress container any environment variables to configure it. As soon as we linked it up to a properly configured MariaDB container everything just worked.

This is because there's a script inside the WordPress Docker container that actually grabs the MYSQL_ROOT_PASSWORD variable from our wordpress_db container and uses that to connect to WordPress. 

Let's venture out of the official image area a little bit and use a community contributed PhpMyAdmin image. Go ahead and edit docker-compose.yml one more time:


        #
        $ nano docker-compose.yml

Paste the following at the end of the file:


		#
		phpmyadmin:
		  image: corbinu/docker-phpmyadmin
		  links:
			- wordpress_db:mysql
		  ports:
			- 8181:80
		  environment:
			MYSQL_USERNAME: root
			MYSQL_ROOT_PASSWORD: examplepass



Be sure to replace examplepass with the exact same root password from the wordpress_db container you setup earlier.


This grabs docker-phpmyadmin by community member corbinu, links it to our wordpress_db container with the name mysql (meaning from inside the phpmyadmin container references to the hostname mysql will be forwarded to our wordpress_db container), exposes its port 80 on port 8181 of the host system, and finally sets a couple of environment variables with our MariaDB username and password. This image does not automatically grab the MYSQL_ROOT_PASSWORD environment variable from the wordpress_db container's environment the way the wordpress image does. We actually have to copy the MYSQL_ROOT_PASSWORD: examplepass line from the wordpress_db container, and set the username to root.

The complete docker-compose.yml file should now look like this:


		#
		wordpress:
		  image: wordpress
		  links:
			- wordpress_db:mysql
		  ports:
			- 8080:80
		wordpress_db:
		  image: mariadb
		  environment:
			MYSQL_ROOT_PASSWORD: examplepass
		phpmyadmin:
		  image: corbinu/docker-phpmyadmin
		  links:
			- wordpress_db:mysql
		  ports:
			- 8181:80
		  environment:
			MYSQL_USERNAME: root
			MYSQL_ROOT_PASSWORD: examplepass



Now start up the application group again:

$ docker-compose up -d
---

You will see PhpMyAdmin being installed. Once it is finished, visit your server's IP address again (this time using port 8181, e.g. http://123.456.789.123:8181). You'll be greeted by the PhpMyAdmin login screen.

Go ahead and login using username root and password you set in the YAML file, and you'll be able to browse your database. You'll notice that the server includes a wordpress database, which contains all the data from your WordPress install.

You can add as many containers as you like this way and link them all up in any way you please. As you can see, the approach is quite powerful —instead of dealing with the configuration and prerequisites for each individual components and setting them all up on the same server, you get to plug the pieces together like Lego blocks and add components piecemeal. Using tools like Docker Swarm you can even transparently run these containers over multiple servers! That's a bitoutside the scope of this tutorial though. Docker provides some [documentation]((https://docs.docker.com/swarm/install-w-machine/)) on it if you are interested. 



Creating the WordPress Site
+++

Since all the files for your new WordPress site are stored inside your Docker container, what happens to your files when you stop the container and start it again?

By default, the document root for the WordPress container is persistent. This is because the WordPress image from the Docker Hub is configured this way. If you make a change to your WordPress site, stop the application group, and start it again, your website will still have the changes you made.

Let's try it.

Go to your WordPress from a web browser (e.g. http://123.456.789.123:8080). Edit the Hello World! post that already exists. Then, stop all the Docker containers with the following command:

$ docker-compose stop
---

Try loading the WordPress site again. You will see that the website is down. Start the Docker containers again:

$ docker-compose up -d
---

Again, load the WordPress site. You should see your blog site and the change you made earlier. This shows that the changes you make are saved even when the containers are stopped.


Storing the Document Root on the Host Filesystem (Optional)
+++

t is possible to store the document root for WordPress on the host filesystem using a Docker data volume to share files between the host and the container.

Note: For more details on working with Docker data volumes, take a look at the Docker data volumes tutorial.

Let's give it a try. Open up your docker-compose.yml file one more time:


		#
		$ nano ~/wordpress/docker-compose.yml


in the wordpress: section add the following lines:


		#
		wordpress:
		...
		  volumes:
			- ~/wordpress/wp_html:/var/www/html
			...


Stop your currently running docker-compose session:

$ docker-compose stop
---

Remove the existing container so we can map the volume to the host filesystem:

$ docker-compose rm wordpress
---

Start WordPress again:

$ docker-compose up -d
---


Once the prompt returns, WordPress should be up and running again — this time using the host filesystem to store the document root.

If you look in your ~/wordpress directory, you'll see that there is now a wp_html directory in it:


		#
		$ ls ~/wordpress

All of the WordPress source files are inside it. Changes you make will be picked up by the WordPress container in real time.

This experience was a little smoother than it normally would be — the WordPress Docker container is configured to check if /var/www/html is empty or not when it starts and copies files there appropriately. Usually you will have to do this step yourself.


Conclusion

You should have a full WordPress deploy up and running. You should be able to use the same method to deploy quite a wide variety of systems using the images available on the Docker Hub. Be sure to figure out which volumes are persistent and which are not for each container you create.


Comments
+++

December 3, 2015



For the mariadb/mysql section I wanted to preserve the database files, so the following volume pattern for the wordpress_db section worked for me:


		#
		wordpress_db:
		  ......
		 volumes:
			- /somehostpath/dbsubdirname:/var/lib/mysql
		........


This maps the host directory /somehostpath/dbsubdirname to the standard location within the container for mariadb/mysql database directory. The phpMyAdmin worked as expected.



Title: how-to-install-prometheus-using-docker-on-ubuntu-14-04
===
Source: https://www.digitalocean.com/community/tutorials/how-to-install-prometheus-using-docker-on-ubuntu-14-04

Added: Fri Nov 11 13:22:51 CET 2016
Created: PostedJanuary 12, 2016


Introduction
+++

Prometheus is an open source monitoring system and time series database. It addresses many aspects of monitoring such as the generation and collection of metrics, graphing the resulting data on dashboards, and alerting on anomalies. To achieve this, it offers a variety of components that are run separately but used in combination.

Docker provides a way for you to encapsulate server processes using Linux containers (or other encapsulation technologies) so that they are more easily managed and isolated from each other. To learn more about Docker, see The Docker Ecosystem: An Introduction to Common Components.

In this tutorial, we will learn how to install three key components for using Prometheus on Docker. These are:

    - A Prometheus server to collect metrics and query them
    - A Node Exporter to export system metrics in a Prometheus-compatible format
    - Grafana, a web-based graphical dashboard builder that supports Prometheus among other backends

There are many more components in the Prometheus ecosystem, but these three provide a good starting point for using Prometheus.


Step 1 Installing Prometheus
+++

This section will explain how to install the main Prometheus server using Docker. Make sure to install Docker per the Prerequisites section before continuing. The Prometheus server is the central piece of the Prometheus ecosystem and is responsible for collecting and storing metrics as well as processing expression queries and generating alerts.

Docker container images for all Prometheus components are hosted under the prom organization on Docker Hub.

https://hub.docker.com/u/prom/


Running the prom/prometheus Docker image without any further options starts the Prometheus server with an example configuration file located at /etc/prometheus/prometheus.yml inside the container. It also uses a Docker data volume mounted at /prometheus inside the container to store collected metrics data.

https://www.digitalocean.com/community/tutorials/how-to-work-with-docker-data-volumes-on-ubuntu-14-04


This data volume directory is actually a directory on the host which Docker auto-creates when the container is first started. The data inside it is persisted between restarts of the same container.


There are multiple ways for overriding the default configuration file. For example, a custom configuration file may be passed into the container from the host filesystem as a Docker data volume, or you could choose to build a derived Docker container with your own configuration file baked into the container image. In this tutorial, we will choose to pass in a configuration file from the host system.


There are different patterns for organizing the storage of metrics as well. In this tutorial, we will use the Docker image's default behavior of using a Docker data volume to store the metrics. You may alternatively consider creating a data volume container if that suits your needs better.


Note: To learn how to create a Docker data volume, read How To Work with Docker Data Volumes on Ubuntu 14.04.

https://www.digitalocean.com/community/tutorials/how-to-work-with-docker-data-volumes-on-ubuntu-14-04

First, create a minimal Prometheus configuration file on the host filesystem at ~/prometheus.yml:


		#
		$ nano ~/prometheus.yml

Add the following contents to the file (replace your_server_ip with your Droplet's IP address):
~/prometheus.yml


		# A scrape configuration scraping a Node Exporter and the Prometheus server
		# itself.
		scrape_configs:
		  # Scrape Prometheus itself every 5 seconds.
		  - job_name: 'prometheus'
			scrape_interval: 5s
			# target_groups:
			static_configs:
			  - targets: ['localhost:9090']

		  # Scrape the Node Exporter every 5 seconds.
		  - job_name: 'node'
			scrape_interval: 5s
			# target_groups:
			static_configs:
			  - targets: ['192.168.x.y:9100']

MY NOTE:
Had to comment out 'target_groups' in the config above, and replace them with 'static_configs'.
https://prometheus.io/docs/operating/configuration/
Prior to v0.20, target_groups was used instead of static_configs. target_groups can still be used alternatively in v0.20 itself, but not in later versions.



This example configuration makes Prometheus scrape metrics from itself (since Prometheus also exposes metrics about itself in a Prometheus-compatible format) as well as from a Node Exporter, which we will set up later. While Prometheus can connect to itself within its container using the localhost host name, it will need to scrape the Node Exporter using your server's external IP since the Node Exporter will run in a separate container with a separate networking namespace.

Start the Prometheus Docker container with the external configuration file:

$ docker run -d -p 9090:9090 -v ~/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus -config.file=/etc/prometheus/prometheus.yml -storage.local.path=/prometheus -storage.local.memory-chunks=10000
---

The first time you run this command, it will pull the Docker image from the Docker Hub.

This command is quite long and contains many command line options. Let's take a look at it in more detail:

   - The -d option starts the Prometheus container in detached mode, meaning that the container will be started in the background and will not be terminated by pressing CTRL+C.
   - The -p 9090:9090 option exposes Prometheus's web port (9090) and makes it reachable via the external IP address of the host system.
   - The -v [...] option mounts the prometheus.yml configuration file from the host filesystem into the location within the container where Prometheus expects it (/etc/prometheus/prometheus.yml).
   - The -config.file option is set accordingly to the location of the Prometheus configuration file within in the container.
   - The -storage.local.path option configures the metrics storage location within the container.
   - Finally, the -storage.local.memory-chunks option adjusts Prometheus's memory usage to the host system's very small amount of RAM (only 512MB) and small number of stored time series in this tutorial (just under 1000). It instructs Prometheus to keep only 10000 sample chunks in memory (roughly 10 chunks per series), instead of the default of 1048576. This is a value you will definitely need to tune when running Prometheus on a machine with more RAM and when storing more time series. Refer to Prometheus's storage documentation for more details around this.


You can list all running Docker containers using the following command:

$ docker ps
---

For example, you will see something similar to the following for the Prometheus Docker container:


		#
		Output of `docker ps`
		CONTAINER ID        IMAGE                COMMAND                  CREATED             STATUS              PORTS                    NAMES
		6a89ac39911e        prom/prometheus      "/bin/prometheus -con"   2 hours ago         Up 2 hours          0.0.0.0:9090->9090/tcp   stoic_pike


Using the container ID shown in the docker ps output, you may inspect the logs of the running Prometheus server with the command:

$ docker logs container_id
---

In our example, the command would be:


		#
		$ docker logs 6a89ac39911e


To find out where on the host's filesystem the metrics storage volume is stored, you can run the following with your container_id:

$ docker inspect container_id
---

This will output information about your container's configuration, including host paths of any mounted Docker volumes.

Find a section in the output that looks similar to this:


		#
		Output of `docker inspect`
		...
		"Mounts": [
			{
				"Source": "/home/sammy/prometheus.yml",
				"Destination": "/etc/prometheus/prometheus.yml",
				"Mode": "",
				"RW": true
			},
			{
				"Name": "821b0abc470a9c758ff35ed5cff69077423a629566082a605a01d8207d57cd6c",
				"Source": "/var/lib/docker/volumes/821b0abc470a9c758ff35ed5cff69077423a629566082a605a01d8207d57cd6c/_data",
				"Destination": "/prometheus",
				"Driver": "local",
				"Mode": "",
				"RW": true
			}
		],
		...


In this example, the metrics are stored in /var/lib/docker/volumes/821b0abc470a9c758ff35ed5cff69077423a629566082a605a01d8207d57cd6c/_data on the host system. This directory was automatically created by Docker when first starting the Prometheus container. It is mapped into the /prometheus directory in the container. Data in this directory is persisted across restarts of the same container. If you prefer, you may also mount an existing host directory for storing metrics data. See How To Work with Docker Data Volumes on Ubuntu 14.04 for how to achieve this.


You should now be able to reach your Prometheus server at http://your_server_ip:9090/. Verify that it is collecting metrics about itself by heading to http://your_server_ip:9090/status and locating the http://localhost:9090/metrics endpoint for the prometheus job in the Targets section. The State column for this target should show the the target's state as HEALTHY. In contrast, the http://localhost:9100/metrics (Node Exporter) endpoint should still show up as UNHEALTHY since the Node Exporter has not yet been started and thus cannot be scraped.

To summarize, you now have Prometheus running as a Docker container using the custom Prometheus configuration file ~/prometheus.yml, which is located on the host filesystem. The metrics storage is located in the /prometheus directory in the container, which is backed on the host system by the path shown by the docker inspect command explained in this section.


Step 2 Setting up Node Exporter
+++

In this section, we will install the Prometheus Node Exporter. The Node Exporter is a server that exposes Prometheus metrics about the host machine (node) it is running on. This includes metrics about the machine's filesystems, networking devices, processor usage, memory usage, and more.

Note that running the Node Exporter on Docker poses some challenges since its entire purpose is to expose metrics about the host it is running on. If we run it on Docker without further options, Docker's namespacing of resources such as the filesystem and the network devices will cause it to only export metrics about the container's environment, which will differ from the host's environment. Thus it is usually recommended to run the Node Exporter directly on the host system outside of Docker. However, if you have a requirement to manage all your processes using Docker, we will describe a workaround which provides a reasonable approximation for exporting host metrics from within Docker.

To start the Node Exporter on port 9100 using Docker:

$ docker run -d -p 9100:9100 -v "/proc:/host/proc" -v "/sys:/host/sys" -v "/:/rootfs" --net="host" prom/node-exporter -collector.procfs /host/proc -collector.sysfs /host/proc -collector.filesystem.ignored-mount-points "^/(sys|proc|dev|host|etc)($|/)"
---

The following Docker and Node Exporter flags are used to provide a reasonable approximation for the host metrics:


    - On Linux, the Node Exporter gathers most of its metrics from the /proc and /sys filesystems. These filesystems are mounted from the host into the container underneath a /host directory, using Docker's -v flag.
    - Via the Node Exporter's -collector.procfs and -collector.sysfs flags, we instruct the Node Exporter to look for the /proc and /sys filesystems in a non-standard location.
    - To report host filesystem metrics, we also mount the entire root (/) filesystem into the container (at /rootfs), again using Docker's -v flag.
    - Use Node Exporter's -collector.filesystem.ignored-mount-points flag to ignore any other filesystems within the container that do not belong to the host system. This option takes a regular expression of mount points to exclude from the reported metrics.
    - Using the --net=host Docker flag, we place the container into the same network stack as the host, so that reading from files such as /proc/net/dev will yield the same results as on the host (reading from the /proc filesystem mounted in from the host is not sufficient).


Note that some metrics will still differ in comparison to a Node Exporter running directly on the host. Specifically, metrics reported about filesystem usage will have a /rootfs prefix in the value of their mountpoint labels, due to us mounting in the root filesystem under this prefix in the container. There is also no guarantee that the described workarounds will be sufficient for any future Node Exporter features, so run Node Exporter on Docker at your own discretion.


The Prometheus server should now automatically start scraping the Node Exporter. Head to your Prometheus server's status page at http://your_server_ip:9090/status and verify that the http://your_server_ip:9100/metrics target for the node job is now showing a HEALTHY state.

MY_NOTE:
I could not get node to change the status to HEALTHY/UP even though it is available at the specified URL.

Step 3 Setting up Grafana
+++

Finally, we will set up Grafana. Grafana is a graphical dashboard builder that supports Prometheus as a backend to query for data to graph.

Grafana stores its dashboard metadata (such as which dashboards exist and what graphs they should show) in a configurable SQL-based database. Grafana supports using a local file-backed SQLite3 database as well as external database servers such as MySQL or PostgreSQL for this.

In this tutorial, we will use a SQLite3 database backed by a Docker data volume. See How and When to Use Sqlite for more information.


Launch Grafana as a Docker container with an administrator password (admin_password) of your choosing:

$ docker run -d -p 3000:3000 -e "GF_SECURITY_ADMIN_PASSWORD=admin_password" -v ~/grafana_db:/var/lib/grafana grafana/grafana
---


This will download the Grafana Docker image from the Docker Hub and create a new Docker volume placed at ~/grafana_db on the host system and at /var/lib/grafana in the container filesystem. In the container, Grafana will then automatically create and initialize its SQLite3 database at /var/lib/grafana/grafana.db.

The -e flag allows passing environment variables to the process launched inside the Docker container. Here, we use it to set the GF_SECURITY_ADMIN_PASSWORD environment variable to the desired dashboard administrator password, overriding the default password of admin. Environment variables may also be used to override any other Grafana configuration settings. See Using environment variables for more details.

To verify that Grafana is running correctly, head to http://your_server_ip:3000/. The administrator username is admin and the password is the one you chose when starting the Docker container previously.

After logging in, you should see Grafana's main view.

See the Grafana documentation for more on how to get started with Grafana. The Prometheus Grafana documentation also shows how to use Grafana in combination with Prometheus specifically.

Conclusion

Congratulations! You have set up a Prometheus server, a Node Exporter, and Grafana — all using Docker. Even though these are currently all running on the same machine, this is only for demonstration purposes. In production setups, one would usually run the Node Exporter on every monitored machine, multiple Prometheus servers (as needed by the organization), as well as a single Grafana server to graph the data from these servers.

To learn more about using Prometheus in general, refer to its documentation.



Title: how-to-configure-a-continuous-integration-testing-environment-with-docker-and-docker-compose-on-ubuntu-14-04
===
Source: https://www.digitalocean.com/community/tutorials/how-to-configure-a-continuous-integration-testing-environment-with-docker-and-docker-compose-on-ubuntu-14-04

Added: Mon Nov 14 13:28:56 CET 2016
Created: UpdatedNovember 3, 2016


Introduction
+++

Continuous integration (CI) refers to the practice where developers integrate code as often as possible and every commit is tested before and after being merged into a shared repository by an automated build.

CI speeds up your development process and minimizes the risk of critical issues in production, but it is not trivial to set up; automated builds run in a different environment where the installation of runtime dependencies and the configuration of external services might be different than in your local and dev environments.


Docker is a containerization platform which aims to simplify the problems of environment standardization so the deployment of applications can also be standardized (find out more about Docker). For developers, Docker allows you to simulate production environments on local machines by running application components in local containers. These containers are easily automatable using Docker Compose, independently of the application and the underlying OS.

This tutorial uses Docker Compose to demonstrate the automation of CI workflows.

We will create a Dockerized "Hello world" type Python application and a Bash test script. The Python application will require two containers to run: one for the app itself, and a Redis container for storage that's required as a dependency for the app.


Then, the test script will be Dockerized in its own container and the whole testing environment moved to a docker-compose.test.yml file so we can make sure we are running every test execution on a fresh and uniform application environment.

This approach shows how you can build an identical, fresh testing environment for your application, including its dependencies, every time you test it.

Thus, we automate the CI workflows independently of the application under test and the underlying infrastructure.


Step 3 Create the "Hello World" Python Application
+++

In this step we will create a simple Python application as an example of the type of application you can test with this setup.

Create a fresh folder for our application by executing:


		#
		cd ~
		mkdir hello_world
		cd hello_world


Edit a new file app.py with nano:

		
		#
		nano app.py


Add the following content:


		
		from flask import Flask
		from redis import Redis

		app = Flask(__name__)
		redis = Redis(host="redis")

		@app.route("/")
		def hello():
			visits = redis.incr('counter')
			html = "<h3>Hello World!</h3>" \
				   "<b>Visits:</b> {visits}" \
				   "<br/>"
			return html.format(visits=visits)

		if __name__ == "__main__":
			app.run(host="0.0.0.0", port=80)



app.py is a web application based on Flask that connects to a Redis data service. The line visits = redis.incr('counter') increases the number of visits and persists this value in Redis. Finally, a Hello World message with the number of visits is returned in HTML.

Our application has two dependencies, Flask and Redis, which you can see in the first two lines. These dependencies must be defined before we can execute the application. Edit a new file:


		#
		nano requirements.txt


Add the contents:

		
		Flask
		Redis


Step 4  Dockerize the "Hello World" Application
+++

Docker uses a file called Dockerfile to indicate the required steps to build a Docker image for a given application. Edit a new file:


		#
		nano Dockerfile

Add the following contents:


		#
		FROM python:2.7

		WORKDIR /app

		ADD requirements.txt /app/requirements.txt
		RUN pip install -r requirements.txt

		ADD app.py /app/app.py

		EXPOSE 80

		CMD ["python", "app.py"]



Let's analyze the meaning of each line:


    - FROM python:2.7: indicates that our "Hello World" application image is built from the official python:2.7 Docker image
    - WORKDIR /app: sets the working directory inside of the Docker image to /app
    - ADD requirements.txt /app/requirements.txt: adds the file requirements.txt to our Docker image
    - RUN pip install -r requirements.txt: installs the application pip dependencies
    - ADD app.py /app/app.py: adds our application source code to the Docker image
    - EXPOSE 80: indicates that our application can be reached at port 80 (the standard public web port)
    - CMD ["python", "app.py"]: the command that starts our application


This Dockerfile file has all the information needed to build the main component of our "Hello World" application.



The Dependency

Now we get to the more sophisticated part of the example. Our application requires Redis as an external service. This is the type of dependency that could be difficult to set up in an identical way every time in a traditional Linux environment, but with Docker Compose we can set it up in a repeatable way every time.


Let's create a docker-compose.yml file to start using Docker Compose.

Edit a new file:


		#
		nano docker-compose.yml

Add the following contents:


		#
		web:
		  build: .
		  dockerfile: Dockerfile
		  links:
			- redis
		  ports:
			- "80:80"
		redis:
		  image: redis



This Docker Compose file indicates how to spin up the "Hello World" application locally in two Docker containers.


It defines two containers, web and redis.


    - web uses the current folder for the build context, and builds our Python application from the Dockerfile file we just created. This is a local Docker image we made just for our Python application. It defines a link to the redis container in order to have access to the redis container IP. It also makes port 80 publicly accessible from the Internet using your Ubuntu server's public IP

    - redis is executed from a standard public Docker image, named redis


Step 5  Deploy the "Hello World" Application
+++

In this step, we'll deploy the application, and by the end it will be accessible over the Internet. For the purposes of your deployment workflow, you could consider this to be either a dev, staging, or production environment, since you could deploy the application the same way numerous times.

The docker-compose.yml and Dockerfile files allow you to automate the deployment of local environments by executing:

$ docker-compose -f ~/hello_world/docker-compose.yml build
---

$ docker-compose -f ~/hello_world/docker-compose.yml up -d
---

The first line builds our local application image from the Dockerfile file. The second line runs the web and redis containers in daemon mode (-d), as specified in the docker-compose.yml file.

Check that the application containers have been created by executing:

$ docker ps
---


This should show two running containers, named helloworld_web_1 and helloworld_redis_1.

Let's check to see that the application is up. We can get the IP of the helloworld_web_1 container by executing:


		#
		WEB_APP_IP=$(docker inspect --format '{{ .NetworkSettings.IPAddress }}' helloworld_web_1)
		echo $WEB_APP_IP

Check that the web application is returning the proper message:


		#
		curl http://${WEB_APP_IP}:80

This should return something like:


		#
		<h3>Hello World!</h3><b>Visits:</b> 1<br/>


The number of visits is incremented every time you hit this endpoint. You can also access the "Hello World" application from your browser by visiting the public IP address of your Ubuntu server.

How to Customize for Your Own Application

The key to setting up your own application is to put your app in its own Docker container, and to run each dependency from its own container. Then, you can define the relationships between the containers with Docker Compose, as demonstrated in the example. Docker Compose is covered in greater detail in this Docker Compose article.

For another example of how to get an application running across several containers, read this article about running WordPress and phpMyAdmin with Docker Compose.


Step 6  Create the Test Script
+++

Now we'll create a test script for our Python application. This will be a simple script that checks the application's HTTP output. The script is an example of the type of test that you might want to run as part of your continuous integration deployment process.

Edit a new file:


		#
		nano test.sh

Add the following contents:


		sleep 5
		if curl web | grep -q '<b>Visits:</b> '; then
		  echo "Tests passed!"
		  exit 0
		else
		  echo "Tests failed!"
		  exit 1
		fi


test.sh tests for basic web connectivity of our "Hello World" application. It uses cURL to retrieve the number of visits and reports on whether the test was passed or not.

Step 7  Create the Testing Environment
+++

In order to test our application, we need to deploy a testing environment. And, we want to make sure it's identical to the live application environment we created in Step 5.

First, we need to Dockerize our testing script by creating a new Dockerfile file. Edit a new file:


		#
		nano Dockerfile.test

Add the following contents:


		#
		FROM ubuntu:trusty

		RUN apt-get update && apt-get install -yq curl && apt-get clean

		WORKDIR /app

		ADD test.sh /app/test.sh

		CMD ["bash", "test.sh"]


Dockerfile.test extends the official ubuntu:trusty image to install the curl dependency, adds tests.sh to the image filesystem, and indicates the CMD command that executes the test script with Bash.

Once our tests are Dockerized, they can be executed in a replicable and agnostic way.

The next step is to link our testing container to our "Hello World" application. Here is where Docker Compose comes to the rescue again. Edit a new file:


		#
		nano docker-compose.test.yml


Add the following contents:


		#
		sut:
		  build: .
		  dockerfile: Dockerfile.test
		  links:
			- web
		web:
		  build: .
		  dockerfile: Dockerfile
		  links:
			- redis
		redis:
		  image: redis



The second half of the Docker Compose file deploys the main web application and its redis dependency in the same way as the previous docker-compose.yml file. This is the part of the file that specifies the web and redis containers. The only difference is that the web container no longer exposes port 80, so the application won't be available over the public Internet during the tests. So, you can see that we're building the application and its dependencies exactly the same way as they are in the live deployment.


The docker-compose.test.yml file also defines a sut container (named for system under tests) that is responsible for executing our integration tests. The sut container specifies the current directory as our build directory and specifies our Dockerfile.test file. It links to the web container so the application container's IP address is accessible to our test.sh script.


How to Customize for Your Own Application

Note that docker-compose.test.yml might include dozens of external services and multiple test containers. Docker will be able to run all these dependencies on a single host because every container shares the underlying OS.

If you have more tests to run on your application, you can create additional Dockerfiles for them, similar to the Dockerfile.test file shown above.

Then, you can add additional containers below the sut container in the docker-compose.test.yml file, referencing the additional Dockerfiles.



Step 8  Test the "Hello World" Application
+++

Finally, extending the Docker ideas from local environments to testing environments, we have an automated way of testing our application using Docker by executing:

$ docker-compose -f ~/hello_world/docker-compose.test.yml -p ci build
---


This command builds the local images needed by docker-compose.test.yml. Note that we are using -f to point to docker-compose.test.yml and -p to indicate a specific project name.

Now spin up your fresh testing environment by executing:

$ docker-compose -f ~/hello_world/docker-compose.test.yml -p ci up -d
---


Check the output of the sut container by executing:

$ docker logs -f ci_sut_1
---


		#
		% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
										 Dload  Upload   Total   Spent    Left  Speed
		100    42  100    42    0     0   3902      0 --:--:-- --:--:-- --:--:--  4200
		Tests passed!


And finally, check the exit code of the sut container to verify if your tests have passed:

$ docker wait ci_sut_1
---

		#
		Output
		0


After the execution of this command, the value of $? will be 0 if the tests passed. Otherwise, our application tests failed.

Note that other CI tools can clone our code repository and execute these few commands to verify if tests are passing with the latest bits of your application without worrying about runtime dependencies or external service configurations.

That's it! We've successfully run our test in a freshly built environment identical to our production environment.



Conclusion
+++

Thanks to Docker and Docker Compose, we have been able to automate how to build an application (Dockerfile), how to deploy a local environment (docker-compose.yml), how to build a testing image (Dockerfile.test), and how to execute (integration) tests (docker-compose.test.yml) for any application.

In particular, the advantages of using the docker-compose.test.yml file for testing are that the testing process is:

    - Automatable: the way a tool executes the docker-compose.test.yml is independent of the application under test
    - Light-weight: hundreds of external services can be deployed on a single host, simulating complex (integration) test environments
    - Agnostic: avoid CI provider lock-in, and your tests can run in any infrastructure and on any OS which supports Docker
    - Immutable: tests passing on your local machine will pass in your CI tool

This tutorial shows an example of how to test a simple "Hello World" application.

Now it's time to use your own application files, Dockerize your own application test scripts, and create your own docker-compose.test.yml to test your application in a fresh and immutable environment.



